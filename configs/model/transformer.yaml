type: transformer

architecture:
  d_model: 128
  nhead: 8
  num_encoder_layers: 4
  dim_feedforward: 512
  dropout: 0.1
  activation: gelu
  norm_first: true  # Pre-norm (more stable)

input:
  sequence_length: 63  # ~3 months daily
  num_features: null  # Auto-detected from feature engine

heads:
  distribution:
    type: gaussian  # gaussian, student_t
  direction:
    num_classes: 3  # up, down, flat
    threshold: 0.005  # +/-0.5% defines flat zone
  volatility:
    enabled: true

training:
  epochs: 100
  batch_size: 64
  learning_rate: 1.0e-4
  weight_decay: 1.0e-5
  warmup_steps: 1000
  max_grad_norm: 1.0
  patience: 10  # Early stopping patience

loss:
  distribution_weight: 1.0
  direction_weight: 0.3
  volatility_weight: 0.3
