encoder:
  d_model: 128
  nhead: 8
  num_layers: 4
  dim_feedforward: 512
  dropout: 0.1
  activation: gelu

tokenizer:
  patch_size: 5  # 5-day patches
  stride: 5  # Non-overlapping

masking:
  mask_ratio: 0.15  # 15% of patches masked
  min_ratio: 0.10
  max_ratio: 0.30

training:
  epochs: 50
  batch_size: 128
  learning_rate: 1.0e-4
  weight_decay: 1.0e-5
  warmup_steps: 500
