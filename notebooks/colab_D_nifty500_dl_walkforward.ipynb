{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "file_extension": ".py",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook D: NIFTY 500 DL Training + Walk-Forward Validation\n",
    "**Run on Colab Pro+ H100** | Trains Transformer & TFT on broader NIFTY 500 universe, runs walk-forward validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === ENVIRONMENT SETUP ===\n",
    "import subprocess, sys, os\n",
    "\n",
    "# Reset working directory (prevents getcwd bug on re-run)\n",
    "os.chdir(\"/content\")\n",
    "\n",
    "# Clean and re-clone for fresh code\n",
    "if os.path.exists('/content/quant-lab'):\n",
    "    print(\"Removing existing quant-lab directory...\")\n",
    "    subprocess.run(['rm', '-rf', '/content/quant-lab'])\n",
    "\n",
    "print(\"Cloning repository...\")\n",
    "result = subprocess.run(\n",
    "    ['git', 'clone', 'https://github.com/Mohit1053/quant-lab.git', '/content/quant-lab'],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if result.returncode != 0:\n",
    "    print(f\"Clone failed: {result.stderr}\")\n",
    "    raise RuntimeError(\"Git clone failed\")\n",
    "print(\"Clone successful.\")\n",
    "\n",
    "os.chdir('/content/quant-lab')\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-e', '.'], check=True)\n",
    "print(\"Package installed.\")\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "from pathlib import Path\n",
    "DRIVE_DIR = Path('/content/drive/MyDrive/quant_lab')\n",
    "for d in ['data/features', 'data/cleaned', 'data/raw', 'data/universe_cache',\n",
    "          'outputs/models/transformer', 'outputs/models/tft',\n",
    "          'outputs/walk_forward/ridge', 'outputs/walk_forward/transformer']:\n",
    "    (DRIVE_DIR / d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.cuda.get_device_name(0)\n",
    "    mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu} ({mem:.1f} GB) | BF16: {torch.cuda.is_bf16_supported()}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === FIX NUMPY/SCIPY VERSIONS (Colab ships newer incompatible versions) ===\n",
    "!pip uninstall -y numpy pandas scipy scikit-learn\n",
    "!pip install --no-cache-dir numpy==1.26.4 pandas==2.2.2 scipy==1.11.4 scikit-learn==1.4.2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "zinrb211dme",
   "source": "# === LOAD NIFTY 500 DATA ===\n# Priority: 1) Google Drive cache  2) File upload  3) NSE pipeline (may fail on Colab IPs)\nfrom pathlib import Path\nimport shutil, subprocess, sys, time\nimport pandas as pd\n\ndrive_features = DRIVE_DIR / 'data/features/nifty500_features.parquet'\nlocal_features = Path('data/features/nifty500_features.parquet')\nPath('data/features').mkdir(parents=True, exist_ok=True)\nPath('data/cleaned').mkdir(parents=True, exist_ok=True)\n\nloaded = False\n\n# --- Option 1: Load from Google Drive (fastest, recommended for 184MB file) ---\nif drive_features.exists():\n    shutil.copy(drive_features, local_features)\n    src = DRIVE_DIR / 'data/cleaned/nifty500_cleaned.parquet'\n    if src.exists():\n        shutil.copy(src, 'data/cleaned/nifty500_cleaned.parquet')\n    print(f\"✅ NIFTY 500 data loaded from Google Drive! ({drive_features.stat().st_size/1e6:.0f} MB)\")\n    loaded = True\n\n# --- Option 2: Upload from local machine (may timeout for >100MB) ---\nif not loaded:\n    print(\"⚠️  No NIFTY 500 data on Drive at:\")\n    print(f\"    {drive_features}\")\n    print()\n    print(\"  RECOMMENDED: Upload nifty500_features.parquet to Google Drive:\")\n    print(\"    1. Go to drive.google.com\")\n    print(\"    2. Navigate to My Drive/quant_lab/data/features/\")\n    print(\"    3. Upload nifty500_features.parquet (184 MB)\")\n    print(\"    4. Re-run this cell\")\n    print()\n    print(\"  Or try direct upload (may timeout for large files)...\")\n    try:\n        from google.colab import files\n        uploaded = files.upload()\n        for fname, content in uploaded.items():\n            if 'nifty500' in fname and fname.endswith('.parquet'):\n                with open(str(local_features), 'wb') as f:\n                    f.write(content)\n                (DRIVE_DIR / 'data/features').mkdir(parents=True, exist_ok=True)\n                shutil.copy(local_features, drive_features)\n                print(f\"✅ Uploaded {fname} ({len(content)/1e6:.1f} MB) — saved to Drive for next time!\")\n                loaded = True\n                break\n        if not loaded and uploaded:\n            print(\"❌ No nifty500 parquet found in uploaded files.\")\n    except Exception as e:\n        print(f\"Upload skipped/failed: {e}\")\n\n# --- Option 3: Try NSE pipeline (usually fails from Colab) ---\nif not loaded:\n    print(\"\\n⏳ Attempting NSE pipeline download (may fail if NSE blocks Colab IPs)...\")\n    try:\n        subprocess.run(\n            [sys.executable, 'scripts/run_pipeline.py', 'data.universe.name=nifty500'],\n            check=True, timeout=600,\n        )\n        if local_features.exists():\n            (DRIVE_DIR / 'data/features').mkdir(parents=True, exist_ok=True)\n            shutil.copy(local_features, drive_features)\n            clean_src = Path('data/cleaned/nifty500_cleaned.parquet')\n            if clean_src.exists():\n                (DRIVE_DIR / 'data/cleaned').mkdir(parents=True, exist_ok=True)\n                shutil.copy(clean_src, DRIVE_DIR / 'data/cleaned/nifty500_cleaned.parquet')\n            print(\"✅ NIFTY 500 data downloaded and saved to Drive!\")\n            loaded = True\n    except Exception as e:\n        print(f\"❌ NSE pipeline failed: {e}\")\n        print(\"   This is expected — NSE blocks Colab datacenter IPs.\")\n        print(\"   Upload nifty500_features.parquet to Google Drive and re-run this cell.\")\n\nif not loaded:\n    raise FileNotFoundError(\n        \"Could not load NIFTY 500 data.\\n\"\n        \"Upload nifty500_features.parquet to Google Drive at:\\n\"\n        f\"  {drive_features}\\n\"\n        \"Then re-run this cell.\"\n    )\n\ndf = pd.read_parquet(local_features)\nprint(f\"\\nNIFTY 500: {df.shape[0]:,} rows, {df['ticker'].nunique()} tickers, {df['date'].nunique()} trading days\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === LOAD NIFTY 500 DATA ===\n# Priority: 1) Google Drive cache  2) File upload  3) NSE pipeline (may fail on Colab IPs)\nfrom pathlib import Path\nimport shutil, subprocess, sys, time\nimport pandas as pd\n\ndrive_features = DRIVE_DIR / 'data/features/nifty500_features.parquet'\nlocal_features = Path('data/features/nifty500_features.parquet')\nPath('data/features').mkdir(parents=True, exist_ok=True)\nPath('data/cleaned').mkdir(parents=True, exist_ok=True)\n\nloaded = False\n\n# --- Option 1: Load from Google Drive (fastest) ---\nif drive_features.exists():\n    shutil.copy(drive_features, local_features)\n    src = DRIVE_DIR / 'data/cleaned/nifty500_cleaned.parquet'\n    if src.exists():\n        shutil.copy(src, 'data/cleaned/nifty500_cleaned.parquet')\n    print(\"✅ NIFTY 500 data loaded from Google Drive!\")\n    loaded = True\n\n# --- Option 2: Upload from local machine ---\nif not loaded:\n    print(\"⚠️  No NIFTY 500 data on Drive.\")\n    print(\"    NSE blocks Colab IPs, so we'll use file upload instead.\")\n    print(\"    Upload 'nifty500_features.parquet' from your local machine.\")\n    print(\"    (Located at: data/features/nifty500_features.parquet)\\n\")\n    try:\n        from google.colab import files\n        uploaded = files.upload()  # User picks the file\n        for fname, content in uploaded.items():\n            if 'nifty500' in fname and fname.endswith('.parquet'):\n                with open(str(local_features), 'wb') as f:\n                    f.write(content)\n                # Also save to Drive for future runs\n                (DRIVE_DIR / 'data/features').mkdir(parents=True, exist_ok=True)\n                shutil.copy(local_features, drive_features)\n                print(f\"✅ Uploaded {fname} ({len(content)/1e6:.1f} MB) — saved to Drive for next time!\")\n                loaded = True\n                break\n        if not loaded:\n            print(\"❌ No nifty500 parquet found in uploaded files.\")\n    except Exception as e:\n        print(f\"Upload failed: {e}\")\n\n# --- Option 3: Try NSE pipeline (may fail from Colab) ---\nif not loaded:\n    print(\"\\nAttempting NSE pipeline download (may fail if NSE blocks Colab IPs)...\")\n    try:\n        subprocess.run(\n            [sys.executable, 'scripts/run_pipeline.py', 'data.universe.name=nifty500'],\n            check=True, timeout=600,\n        )\n        if local_features.exists():\n            (DRIVE_DIR / 'data/features').mkdir(parents=True, exist_ok=True)\n            shutil.copy(local_features, drive_features)\n            clean_src = Path('data/cleaned/nifty500_cleaned.parquet')\n            if clean_src.exists():\n                (DRIVE_DIR / 'data/cleaned').mkdir(parents=True, exist_ok=True)\n                shutil.copy(clean_src, DRIVE_DIR / 'data/cleaned/nifty500_cleaned.parquet')\n            print(\"✅ NIFTY 500 data downloaded and saved to Drive!\")\n            loaded = True\n    except Exception as e:\n        print(f\"❌ NSE pipeline failed: {e}\")\n        print(\"   This is expected — NSE blocks Colab datacenter IPs.\")\n        print(\"   Please upload nifty500_features.parquet manually and re-run this cell.\")\n\nif not loaded:\n    raise FileNotFoundError(\n        \"Could not load NIFTY 500 data. Please upload nifty500_features.parquet \"\n        \"from your local machine (data/features/nifty500_features.parquet).\"\n    )\n\ndf = pd.read_parquet(local_features)\nprint(f\"\\nNIFTY 500: {df.shape[0]:,} rows, {df['ticker'].nunique()} tickers, {df['date'].nunique()} trading days\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Transformer on NIFTY 500"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === TRAIN TRANSFORMER ON NIFTY 500 ===\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from quant_lab.utils.seed import set_global_seed\n",
    "from quant_lab.utils.device import get_device\n",
    "from quant_lab.data.datasets import TemporalSplit\n",
    "from quant_lab.data.datamodule import QuantDataModule, DataModuleConfig\n",
    "from quant_lab.models.transformer.model import TransformerForecaster, TransformerConfig, MultiTaskLoss\n",
    "from quant_lab.training.trainer import Trainer, TrainerConfig\n",
    "\n",
    "set_global_seed(42)\n",
    "device = get_device()\n",
    "\n",
    "base_cols = {'date', 'ticker', 'open', 'high', 'low', 'close', 'volume', 'adj_close'}\n",
    "feature_cols = [c for c in df.columns if c not in base_cols]\n",
    "split = TemporalSplit(train_end='2021-12-31', val_end='2023-06-30')\n",
    "\n",
    "dm = QuantDataModule(\n",
    "    df, feature_cols, split,\n",
    "    DataModuleConfig(sequence_length=63, target_col='log_return_1d', batch_size=128, num_workers=2),\n",
    ")\n",
    "dm.setup()\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}, Features: {dm.num_features}\")\n",
    "\n",
    "model_cfg = TransformerConfig(\n",
    "    num_features=dm.num_features, d_model=128, nhead=8,\n",
    "    num_encoder_layers=4, dim_feedforward=512, dropout=0.1,\n",
    "    direction_weight=0.3, volatility_weight=0.3,\n",
    ")\n",
    "model = TransformerForecaster(model_cfg)\n",
    "loss_fn = MultiTaskLoss(model_cfg)\n",
    "print(f\"Transformer params: {model.count_parameters():,}\")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    epochs=100, learning_rate=1e-4, weight_decay=1e-5,\n",
    "    warmup_steps=2000, patience=10, mixed_precision=True,\n",
    "    checkpoint_dir='outputs/models/transformer',\n",
    ")\n",
    "trainer = Trainer(model, loss_fn, trainer_config, device)\n",
    "\n",
    "start = time.time()\n",
    "trainer.fit(train_loader, val_loader)\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\nTransformer training done in {elapsed/60:.1f} min\")\n",
    "\n",
    "# Save to Drive\n",
    "for f in Path('outputs/models/transformer').glob('*.pt'):\n",
    "    shutil.copy(f, DRIVE_DIR / 'outputs/models/transformer' / f.name)\n",
    "print(\"Transformer saved to Drive!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train TFT on NIFTY 500 (Smaller Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === TRAIN TFT ON NIFTY 500 (SMALLER ARCHITECTURE) ===\n",
    "from quant_lab.models.tft.model import TFTForecaster, TFTConfig\n",
    "\n",
    "set_global_seed(42)\n",
    "\n",
    "# Smaller TFT to prevent mode collapse (d_model=32 not 128, dropout=0.3 not 0.1)\n",
    "tft_cfg = TFTConfig(\n",
    "    num_features=dm.num_features, d_model=32, nhead=4,\n",
    "    num_encoder_layers=1, lstm_layers=1, lstm_hidden=32,\n",
    "    grn_hidden=16, dropout=0.3,\n",
    "    direction_weight=0.3, volatility_weight=0.3,\n",
    ")\n",
    "model = TFTForecaster(tft_cfg)\n",
    "loss_cfg = TransformerConfig(num_features=dm.num_features, direction_weight=0.3, volatility_weight=0.3)\n",
    "loss_fn = MultiTaskLoss(loss_cfg)\n",
    "print(f\"TFT params: {sum(p.numel() for p in model.parameters()):,} (small arch to prevent mode collapse)\")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    epochs=100, learning_rate=3e-4, weight_decay=1e-3,\n",
    "    warmup_steps=500, patience=15, mixed_precision=True,\n",
    "    checkpoint_dir='outputs/models/tft',\n",
    ")\n",
    "trainer = Trainer(model, loss_fn, trainer_config, device)\n",
    "\n",
    "start = time.time()\n",
    "trainer.fit(train_loader, val_loader)\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\nTFT training done in {elapsed/60:.1f} min\")\n",
    "\n",
    "# Verify no mode collapse\n",
    "model.eval()\n",
    "x = torch.randn(20, 63, dm.num_features).to(device)\n",
    "with torch.no_grad():\n",
    "    preds = model.predict_returns(x)\n",
    "print(f\"Signal std: {preds.std():.6f} (should be >> 0)\")\n",
    "print(f\"Signal range: [{preds.min():.6f}, {preds.max():.6f}]\")\n",
    "\n",
    "# Save to Drive\n",
    "Path('outputs/models/tft').mkdir(parents=True, exist_ok=True)\n",
    "for f in Path('outputs/models/tft').glob('*.pt'):\n",
    "    shutil.copy(f, DRIVE_DIR / 'outputs/models/tft' / f.name)\n",
    "print(\"TFT saved to Drive!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk-Forward Validation (Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === WALK-FORWARD VALIDATION (TRANSFORMER) ===\n",
    "from quant_lab.backtest.walk_forward import WalkForwardEngine, WalkForwardConfig, WindowType\n",
    "from quant_lab.backtest.engine import BacktestConfig\n",
    "from quant_lab.data.datasets import create_flat_datasets\n",
    "\n",
    "# Prepare prices\n",
    "prices_df = df[['date', 'ticker', 'adj_close']].copy()\n",
    "\n",
    "# Walk-forward config\n",
    "wf_config = WalkForwardConfig(\n",
    "    window_type=WindowType.EXPANDING,\n",
    "    train_days=756, val_days=126, test_days=126,\n",
    "    step_days=126, min_train_days=504,\n",
    ")\n",
    "backtest_cfg = BacktestConfig(initial_capital=1_000_000, rebalance_frequency=5, top_n=10)\n",
    "\n",
    "# Transformer factory\n",
    "def make_transformer_factory():\n",
    "    def factory(split, feature_df, feat_cols):\n",
    "        dm = QuantDataModule(\n",
    "            feature_df, feat_cols, split,\n",
    "            DataModuleConfig(sequence_length=63, target_col='log_return_1d', batch_size=128, num_workers=2),\n",
    "        )\n",
    "        dm.setup()\n",
    "        tl = dm.train_dataloader()\n",
    "        vl = dm.val_dataloader()\n",
    "        if tl is None:\n",
    "            return None, pd.DataFrame()\n",
    "\n",
    "        cfg = TransformerConfig(\n",
    "            num_features=dm.num_features, d_model=128, nhead=8,\n",
    "            num_encoder_layers=4, dim_feedforward=512, dropout=0.1,\n",
    "            direction_weight=0.3, volatility_weight=0.3,\n",
    "        )\n",
    "        m = TransformerForecaster(cfg)\n",
    "        loss_fn = MultiTaskLoss(cfg)\n",
    "        tc = TrainerConfig(\n",
    "            epochs=30, learning_rate=1e-4, weight_decay=1e-5,\n",
    "            warmup_steps=1000, patience=5, mixed_precision=True,\n",
    "            checkpoint_dir='outputs/walk_forward/transformer',\n",
    "        )\n",
    "        t = Trainer(m, loss_fn, tc, device)\n",
    "        t.fit(tl, vl)\n",
    "\n",
    "        test_loader = dm.test_dataloader()\n",
    "        if test_loader is None:\n",
    "            return m, pd.DataFrame()\n",
    "        m.eval()\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for x, _ in test_loader:\n",
    "                x = x.to(device)\n",
    "                all_preds.append(m.predict_returns(x).cpu().numpy())\n",
    "        test_preds = np.concatenate(all_preds)\n",
    "\n",
    "        datasets = create_flat_datasets(feature_df, feat_cols, split, target_col='log_return_1d')\n",
    "        _, _, meta_test = datasets['test']\n",
    "        meta_test = meta_test.iloc[-len(test_preds):]\n",
    "        signals = meta_test.copy()\n",
    "        signals['signal'] = test_preds\n",
    "        return m, signals\n",
    "    return factory\n",
    "\n",
    "print(\"Starting Transformer walk-forward (this takes 1-2 hours)...\")\n",
    "start = time.time()\n",
    "engine = WalkForwardEngine(wf_config, backtest_cfg)\n",
    "wf_result = engine.run(df, feature_cols, prices_df, make_transformer_factory())\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\nWalk-forward done in {elapsed/60:.1f} min ({len(wf_result.fold_results)} folds)\")\n",
    "print(f\"\\nAggregate metrics:\")\n",
    "for k, v in wf_result.aggregate_metrics.items():\n",
    "    if 'return' in k or 'cagr' in k or 'drawdown' in k:\n",
    "        print(f\"  {k:25s}: {v:>10.2%}\")\n",
    "    else:\n",
    "        print(f\"  {k:25s}: {v:>10.4f}\")\n",
    "\n",
    "print(f\"\\nPer-fold:\")\n",
    "display_cols = ['fold', 'test_start', 'test_end', 'sharpe', 'total_return', 'max_drawdown']\n",
    "available = [c for c in display_cols if c in wf_result.per_fold_metrics.columns]\n",
    "print(wf_result.per_fold_metrics[available].to_string(index=False))\n",
    "\n",
    "# Save to Drive\n",
    "wf_out = DRIVE_DIR / 'outputs/walk_forward/transformer'\n",
    "wf_out.mkdir(parents=True, exist_ok=True)\n",
    "wf_result.per_fold_metrics.to_csv(wf_out / 'per_fold_metrics.csv', index=False)\n",
    "wf_result.aggregate_equity.to_frame('equity').to_parquet(wf_out / 'aggregate_equity.parquet')\n",
    "print(f\"\\nResults saved to Drive!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"NOTEBOOK D COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nAll outputs on Drive:\")\n",
    "for d in ['outputs/models/transformer', 'outputs/models/tft', 'outputs/walk_forward/transformer']:\n",
    "    p = DRIVE_DIR / d\n",
    "    if p.exists():\n",
    "        for f in sorted(p.glob('*')):\n",
    "            if f.is_file():\n",
    "                print(f\"  {f.relative_to(DRIVE_DIR)}: {f.stat().st_size/1e6:.1f} MB\")\n",
    "print(f\"\\nWalk-forward Sharpe: {wf_result.aggregate_metrics.get('sharpe', 'N/A'):.4f}\")\n",
    "print(f\"Walk-forward CAGR: {wf_result.aggregate_metrics.get('cagr', 'N/A'):.2%}\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}