{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "file_extension": ".py",
   "nbconvert_exporter": "python"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100",
   "gpuClass": "premium"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook E: Optuna Hyperparameter Sweep on NIFTY 500\n",
    "**Run on Colab Pro+ H100** | Finds optimal hyperparameters for Transformer & TFT on broader universe"
   ],
   "id": "e0_title"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === ENVIRONMENT SETUP ===\nimport subprocess, sys, os\n\n# Reset working directory (prevents getcwd bug on re-run)\nos.chdir(\"/content\")\n\n# Clean and re-clone for fresh code\nif os.path.exists('/content/quant-lab'):\n    print(\"Removing existing quant-lab directory...\")\n    subprocess.run(['rm', '-rf', '/content/quant-lab'])\n\nprint(\"Cloning repository...\")\nresult = subprocess.run(\n    ['git', 'clone', 'https://github.com/Mohit1053/quant-lab.git', '/content/quant-lab'],\n    capture_output=True, text=True,\n)\nif result.returncode != 0:\n    print(f\"Clone failed: {result.stderr}\")\n    raise RuntimeError(\"Git clone failed\")\nprint(\"Clone successful.\")\n\nos.chdir('/content/quant-lab')\nsubprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-e', '.'], check=True)\nprint(\"Package installed.\")\n\n# Install optuna\nsubprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'optuna'], check=True)\nprint(\"Optuna installed.\")\n\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=False)\n\nfrom pathlib import Path\nDRIVE_DIR = Path('/content/drive/MyDrive/quant_lab')\nfor d in ['data/features', 'data/cleaned', 'outputs/sweep/transformer', 'outputs/sweep/tft']:\n    (DRIVE_DIR / d).mkdir(parents=True, exist_ok=True)\n\nimport torch\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_name(0)\n    mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {gpu} ({mem:.1f} GB)\")\nelse:\n    print(\"WARNING: No GPU!\")",
   "outputs": [],
   "execution_count": null,
   "id": "e1_setup"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === FIX NUMPY/SCIPY VERSIONS (Colab ships newer incompatible versions) ===\n",
    "!pip uninstall -y numpy pandas scipy scikit-learn\n",
    "!pip install --no-cache-dir numpy==1.26.4 pandas==2.2.2 scipy==1.11.4 scikit-learn==1.4.2"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "e2_numpy_fix"
  },
  {
   "cell_type": "markdown",
   "id": "75ljg9cyec3",
   "source": "## Data Loading — Important Note\n\n**NSE blocks Colab datacenter IPs**, so fetching NIFTY 500 tickers directly won't work.\n\n**Recommended**: Upload `nifty500_features.parquet` (184MB) to Google Drive at `My Drive/quant_lab/data/features/` before running the next cell. Or run Notebook D first — it caches data to Drive.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === LOAD NIFTY 500 DATA ===\n# Priority: 1) Google Drive cache  2) File upload  3) NSE pipeline (may fail on Colab IPs)\nimport shutil\nimport pandas as pd\n\ndrive_features = DRIVE_DIR / 'data/features/nifty500_features.parquet'\nlocal_features = Path('data/features/nifty500_features.parquet')\nPath('data/features').mkdir(parents=True, exist_ok=True)\nPath('data/cleaned').mkdir(parents=True, exist_ok=True)\n\nloaded = False\n\n# --- Option 1: Load from Google Drive (fastest, recommended for 184MB file) ---\nif drive_features.exists():\n    shutil.copy(drive_features, local_features)\n    src = DRIVE_DIR / 'data/cleaned/nifty500_cleaned.parquet'\n    if src.exists():\n        shutil.copy(src, 'data/cleaned/nifty500_cleaned.parquet')\n    print(f\"✅ NIFTY 500 data loaded from Google Drive! ({drive_features.stat().st_size/1e6:.0f} MB)\")\n    loaded = True\n\n# --- Option 2: Upload from local machine (may timeout for >100MB) ---\nif not loaded:\n    print(\"⚠️  No NIFTY 500 data on Drive at:\")\n    print(f\"    {drive_features}\")\n    print()\n    print(\"  RECOMMENDED: Upload nifty500_features.parquet to Google Drive:\")\n    print(\"    1. Go to drive.google.com\")\n    print(\"    2. Navigate to My Drive/quant_lab/data/features/\")\n    print(\"    3. Upload nifty500_features.parquet (184 MB)\")\n    print(\"    4. Re-run this cell\")\n    print()\n    print(\"  Or try direct upload (may timeout for large files)...\")\n    try:\n        from google.colab import files\n        uploaded = files.upload()\n        for fname, content in uploaded.items():\n            if 'nifty500' in fname and fname.endswith('.parquet'):\n                with open(str(local_features), 'wb') as f:\n                    f.write(content)\n                (DRIVE_DIR / 'data/features').mkdir(parents=True, exist_ok=True)\n                shutil.copy(local_features, drive_features)\n                print(f\"✅ Uploaded {fname} ({len(content)/1e6:.1f} MB) — saved to Drive for next time!\")\n                loaded = True\n                break\n        if not loaded and uploaded:\n            print(\"❌ No nifty500 parquet found in uploaded files.\")\n    except Exception as e:\n        print(f\"Upload skipped/failed: {e}\")\n\n# --- Option 3: Try NSE pipeline (usually fails from Colab) ---\nif not loaded:\n    print(\"\\n⏳ Attempting NSE pipeline download (may fail if NSE blocks Colab IPs)...\")\n    try:\n        subprocess.run(\n            [sys.executable, 'scripts/run_pipeline.py', 'data.universe.name=nifty500'],\n            check=True, timeout=600,\n        )\n        if local_features.exists():\n            (DRIVE_DIR / 'data/features').mkdir(parents=True, exist_ok=True)\n            shutil.copy(local_features, drive_features)\n            print(\"✅ NIFTY 500 data downloaded and saved to Drive!\")\n            loaded = True\n    except Exception as e:\n        print(f\"❌ NSE pipeline failed: {e}\")\n        print(\"   This is expected — NSE blocks Colab datacenter IPs.\")\n        print(\"   Upload nifty500_features.parquet to Google Drive and re-run this cell.\")\n\nif not loaded:\n    raise FileNotFoundError(\n        \"Could not load NIFTY 500 data.\\n\"\n        \"Upload nifty500_features.parquet to Google Drive at:\\n\"\n        f\"  {drive_features}\\n\"\n        \"Or run Notebook D first to populate Google Drive.\"\n    )\n\ndf = pd.read_parquet(local_features)\nprint(f\"\\nNIFTY 500: {df.shape[0]:,} rows, {df['ticker'].nunique()} tickers\")",
   "outputs": [],
   "execution_count": null,
   "id": "e3_load_data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna Sweep — Transformer\n",
    "TPE sampler with median pruning, 50 trials, 2h timeout"
   ],
   "id": "e4_transformer_header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === OPTUNA SWEEP: TRANSFORMER ===\n",
    "import time\n",
    "import json\n",
    "import optuna\n",
    "import numpy as np\n",
    "import torch\n",
    "from quant_lab.utils.seed import set_global_seed\n",
    "from quant_lab.utils.device import get_device\n",
    "from quant_lab.data.datasets import TemporalSplit\n",
    "from quant_lab.data.datamodule import QuantDataModule, DataModuleConfig\n",
    "from quant_lab.models.transformer.model import TransformerForecaster, TransformerConfig, MultiTaskLoss\n",
    "from quant_lab.training.trainer import Trainer, TrainerConfig\n",
    "\n",
    "set_global_seed(42)\n",
    "device = get_device()\n",
    "\n",
    "base_cols = {'date', 'ticker', 'open', 'high', 'low', 'close', 'volume', 'adj_close'}\n",
    "feature_cols = [c for c in df.columns if c not in base_cols]\n",
    "split = TemporalSplit(train_end='2021-12-31', val_end='2023-06-30')\n",
    "\n",
    "def transformer_objective(trial):\n",
    "    d_model = trial.suggest_categorical('d_model', [64, 128, 256])\n",
    "    nhead = trial.suggest_categorical('nhead', [4, 8])\n",
    "    num_layers = trial.suggest_int('num_encoder_layers', 2, 6)\n",
    "    dim_ff = trial.suggest_categorical('dim_feedforward', [256, 512, 1024])\n",
    "    dropout = trial.suggest_float('dropout', 0.05, 0.3)\n",
    "    lr = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    wd = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    seq_len = trial.suggest_categorical('sequence_length', [21, 42, 63])\n",
    "\n",
    "    dm = QuantDataModule(\n",
    "        df, feature_cols, split,\n",
    "        DataModuleConfig(sequence_length=seq_len, target_col='log_return_1d', batch_size=batch_size, num_workers=2),\n",
    "    )\n",
    "    dm.setup()\n",
    "    tl = dm.train_dataloader()\n",
    "    vl = dm.val_dataloader()\n",
    "    if tl is None:\n",
    "        return float('inf')\n",
    "\n",
    "    cfg = TransformerConfig(\n",
    "        num_features=dm.num_features, d_model=d_model, nhead=nhead,\n",
    "        num_encoder_layers=num_layers, dim_feedforward=dim_ff, dropout=dropout,\n",
    "        direction_weight=0.3, volatility_weight=0.3,\n",
    "    )\n",
    "    model = TransformerForecaster(cfg)\n",
    "    loss_fn = MultiTaskLoss(cfg)\n",
    "\n",
    "    tc = TrainerConfig(\n",
    "        epochs=30, learning_rate=lr, weight_decay=wd,\n",
    "        warmup_steps=500, patience=5, mixed_precision=True,\n",
    "        checkpoint_dir=f'outputs/sweep/transformer/trial_{trial.number}',\n",
    "    )\n",
    "    trainer = Trainer(model, loss_fn, tc, device)\n",
    "    trainer.fit(tl, vl)\n",
    "\n",
    "    return trainer.best_val_loss\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10),\n",
    ")\n",
    "\n",
    "print(\"Starting Transformer sweep (50 trials)...\")\n",
    "start = time.time()\n",
    "study.optimize(transformer_objective, n_trials=50, timeout=7200)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\nSweep done in {elapsed/60:.1f} min\")\n",
    "print(f\"Best val loss: {study.best_value:.6f}\")\n",
    "print(f\"Best params: {json.dumps(study.best_params, indent=2)}\")\n",
    "\n",
    "# Save results\n",
    "out_dir = Path('outputs/sweep/transformer')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "with open(out_dir / 'best_params.json', 'w') as f:\n",
    "    json.dump(study.best_params, f, indent=2)\n",
    "\n",
    "trials_data = [{'number': t.number, 'value': t.value, 'params': t.params} for t in study.trials if t.value is not None]\n",
    "with open(out_dir / 'all_trials.json', 'w') as f:\n",
    "    json.dump(trials_data, f, indent=2)\n",
    "\n",
    "# Save to Drive\n",
    "for f_path in out_dir.glob('*.json'):\n",
    "    shutil.copy(f_path, DRIVE_DIR / 'outputs/sweep/transformer' / f_path.name)\n",
    "print(\"Transformer sweep results saved to Drive!\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "e5_transformer_sweep"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna Sweep — TFT\n",
    "TPE sampler with median pruning, 50 trials, 2h timeout"
   ],
   "id": "e6_tft_header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === OPTUNA SWEEP: TFT ===\n",
    "from quant_lab.models.tft.model import TFTForecaster, TFTConfig\n",
    "\n",
    "def tft_objective(trial):\n",
    "    d_model = trial.suggest_categorical('d_model', [16, 32, 64])\n",
    "    nhead = trial.suggest_categorical('nhead', [2, 4])\n",
    "    num_layers = trial.suggest_int('num_encoder_layers', 1, 3)\n",
    "    lstm_hidden = trial.suggest_categorical('lstm_hidden', [16, 32, 64])\n",
    "    grn_hidden = trial.suggest_categorical('grn_hidden', [8, 16, 32])\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    lr = trial.suggest_float('learning_rate', 1e-4, 1e-3, log=True)\n",
    "    wd = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    seq_len = trial.suggest_categorical('sequence_length', [21, 42, 63])\n",
    "\n",
    "    dm = QuantDataModule(\n",
    "        df, feature_cols, split,\n",
    "        DataModuleConfig(sequence_length=seq_len, target_col='log_return_1d', batch_size=batch_size, num_workers=2),\n",
    "    )\n",
    "    dm.setup()\n",
    "    tl = dm.train_dataloader()\n",
    "    vl = dm.val_dataloader()\n",
    "    if tl is None:\n",
    "        return float('inf')\n",
    "\n",
    "    cfg = TFTConfig(\n",
    "        num_features=dm.num_features, d_model=d_model, nhead=nhead,\n",
    "        num_encoder_layers=num_layers, lstm_layers=1, lstm_hidden=lstm_hidden,\n",
    "        grn_hidden=grn_hidden, dropout=dropout,\n",
    "        direction_weight=0.3, volatility_weight=0.3,\n",
    "    )\n",
    "    model = TFTForecaster(cfg)\n",
    "    loss_cfg = TransformerConfig(num_features=dm.num_features, direction_weight=0.3, volatility_weight=0.3)\n",
    "    loss_fn = MultiTaskLoss(loss_cfg)\n",
    "\n",
    "    tc = TrainerConfig(\n",
    "        epochs=30, learning_rate=lr, weight_decay=wd,\n",
    "        warmup_steps=300, patience=5, mixed_precision=True,\n",
    "        checkpoint_dir=f'outputs/sweep/tft/trial_{trial.number}',\n",
    "    )\n",
    "    trainer = Trainer(model, loss_fn, tc, device)\n",
    "    trainer.fit(tl, vl)\n",
    "\n",
    "    return trainer.best_val_loss\n",
    "\n",
    "tft_study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5),\n",
    ")\n",
    "\n",
    "print(\"Starting TFT sweep (50 trials)...\")\n",
    "start = time.time()\n",
    "tft_study.optimize(tft_objective, n_trials=50, timeout=7200)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\nSweep done in {elapsed/60:.1f} min\")\n",
    "print(f\"Best val loss: {tft_study.best_value:.6f}\")\n",
    "print(f\"Best params: {json.dumps(tft_study.best_params, indent=2)}\")\n",
    "\n",
    "# Save\n",
    "out_dir = Path('outputs/sweep/tft')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "with open(out_dir / 'best_params.json', 'w') as f:\n",
    "    json.dump(tft_study.best_params, f, indent=2)\n",
    "trials_data = [{'number': t.number, 'value': t.value, 'params': t.params} for t in tft_study.trials if t.value is not None]\n",
    "with open(out_dir / 'all_trials.json', 'w') as f:\n",
    "    json.dump(trials_data, f, indent=2)\n",
    "\n",
    "for f_path in out_dir.glob('*.json'):\n",
    "    shutil.copy(f_path, DRIVE_DIR / 'outputs/sweep/tft' / f_path.name)\n",
    "print(\"TFT sweep results saved to Drive!\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "e7_tft_sweep"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Best hyperparameters from both sweeps"
   ],
   "id": "e8_summary_header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"NOTEBOOK E COMPLETE — OPTUNA SWEEPS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTransformer best val loss: {study.best_value:.6f}\")\n",
    "print(f\"Transformer best params: {json.dumps(study.best_params, indent=2)}\")\n",
    "print(f\"\\nTFT best val loss: {tft_study.best_value:.6f}\")\n",
    "print(f\"TFT best params: {json.dumps(tft_study.best_params, indent=2)}\")\n",
    "print(f\"\\nAll results on Drive: {DRIVE_DIR / 'outputs/sweep'}\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "e9_summary"
  }
 ]
}