{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0_title",
   "metadata": {},
   "source": [
    "# Notebook F: Full Indian Market DL Training + Walk-Forward\n",
    "**Run on Colab Pro+ H100** | Trains Transformer & TFT on ALL NSE EQ-series stocks (~1500+ after liquidity filters), runs walk-forward validation\n",
    "\n",
    "**Prerequisites:** Upload `indian_market_features.parquet` to Google Drive at `My Drive/quant_lab/data/features/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ENVIRONMENT SETUP ===\n",
    "import subprocess, sys, os\n",
    "\n",
    "os.chdir('/content')\n",
    "\n",
    "if os.path.exists('/content/quant-lab'):\n",
    "    print('Removing existing quant-lab directory...')\n",
    "    subprocess.run(['rm', '-rf', '/content/quant-lab'])\n",
    "\n",
    "print('Cloning repository...')\n",
    "result = subprocess.run(\n",
    "    ['git', 'clone', 'https://github.com/Mohit1053/quant-lab.git', '/content/quant-lab'],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if result.returncode != 0:\n",
    "    print(f'Clone failed: {result.stderr}')\n",
    "    raise RuntimeError('Git clone failed')\n",
    "print('Clone successful.')\n",
    "\n",
    "os.chdir('/content/quant-lab')\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-e', '.'], check=True)\n",
    "print('Package installed.')\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "from pathlib import Path\n",
    "DRIVE_DIR = Path('/content/drive/MyDrive/quant_lab')\n",
    "for d in ['data/features', 'data/cleaned',\n",
    "          'outputs/models/transformer_fullmkt', 'outputs/models/tft_fullmkt',\n",
    "          'outputs/walk_forward/transformer_fullmkt']:\n",
    "    (DRIVE_DIR / d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.cuda.get_device_name(0)\n",
    "    mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f'GPU: {gpu} ({mem:.1f} GB) | BF16: {torch.cuda.is_bf16_supported()}')\n",
    "else:\n",
    "    print('WARNING: No GPU!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2_numpy_fix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FIX NUMPY/SCIPY VERSIONS ===\n",
    "!pip uninstall -y numpy pandas scipy scikit-learn\n",
    "!pip install --no-cache-dir numpy==1.26.4 pandas==2.2.2 scipy==1.11.4 scikit-learn==1.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3_data_note",
   "metadata": {},
   "source": [
    "## Data Loading — Important Note\n",
    "\n",
    "**NSE blocks Colab datacenter IPs**, so data must be pre-computed locally.\n",
    "\n",
    "**Steps:**\n",
    "1. Run `python scripts/run_indian_market_pipeline.py` on your local machine\n",
    "2. Upload `indian_market_features.parquet` (~500+ MB) to Google Drive at `My Drive/quant_lab/data/features/`\n",
    "3. The cell below will auto-detect it from Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4_load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD FULL MARKET DATA ===\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "drive_features = DRIVE_DIR / 'data/features/indian_market_features.parquet'\n",
    "local_features = Path('data/features/indian_market_features.parquet')\n",
    "Path('data/features').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "loaded = False\n",
    "\n",
    "# --- Option 1: Google Drive ---\n",
    "if drive_features.exists():\n",
    "    shutil.copy(drive_features, local_features)\n",
    "    print(f'Full market data loaded from Drive! ({drive_features.stat().st_size/1e6:.0f} MB)')\n",
    "    loaded = True\n",
    "\n",
    "# --- Option 2: File upload ---\n",
    "if not loaded:\n",
    "    print('No data on Drive. Upload indian_market_features.parquet:')\n",
    "    print('  1. Go to drive.google.com')\n",
    "    print('  2. Navigate to My Drive/quant_lab/data/features/')\n",
    "    print('  3. Upload indian_market_features.parquet')\n",
    "    print('  4. Re-run this cell')\n",
    "    print()\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()\n",
    "        for fname, content in uploaded.items():\n",
    "            if 'indian_market' in fname and fname.endswith('.parquet'):\n",
    "                with open(str(local_features), 'wb') as f:\n",
    "                    f.write(content)\n",
    "                (DRIVE_DIR / 'data/features').mkdir(parents=True, exist_ok=True)\n",
    "                shutil.copy(local_features, drive_features)\n",
    "                print(f'Uploaded {fname} — saved to Drive!')\n",
    "                loaded = True\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f'Upload failed: {e}')\n",
    "\n",
    "if not loaded:\n",
    "    raise FileNotFoundError(\n",
    "        'Could not load full market data.\\n'\n",
    "        'Run scripts/run_indian_market_pipeline.py locally, then upload\\n'\n",
    "        f'indian_market_features.parquet to {drive_features}'\n",
    "    )\n",
    "\n",
    "df = pd.read_parquet(local_features)\n",
    "print(f'\\nFull Indian Market: {df.shape[0]:,} rows, {df[\"ticker\"].nunique()} tickers, {df[\"date\"].nunique()} trading days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5_transformer_header",
   "metadata": {},
   "source": [
    "## Train Transformer on Full Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6_train_transformer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAIN TRANSFORMER ON FULL MARKET ===\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from quant_lab.utils.seed import set_global_seed\n",
    "from quant_lab.utils.device import get_device\n",
    "from quant_lab.data.datasets import TemporalSplit\n",
    "from quant_lab.data.datamodule import QuantDataModule, DataModuleConfig\n",
    "from quant_lab.models.transformer.model import TransformerForecaster, TransformerConfig, MultiTaskLoss\n",
    "from quant_lab.training.trainer import Trainer, TrainerConfig\n",
    "\n",
    "set_global_seed(42)\n",
    "device = get_device()\n",
    "\n",
    "base_cols = {'date', 'ticker', 'open', 'high', 'low', 'close', 'volume', 'adj_close'}\n",
    "feature_cols = [c for c in df.columns if c not in base_cols]\n",
    "split = TemporalSplit(train_end='2021-12-31', val_end='2023-06-30')\n",
    "\n",
    "dm = QuantDataModule(\n",
    "    df, feature_cols, split,\n",
    "    DataModuleConfig(sequence_length=63, target_col='log_return_1d', batch_size=256, num_workers=2),\n",
    ")\n",
    "dm.setup()\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "print(f'Train batches: {len(train_loader)}, Val batches: {len(val_loader)}, Features: {dm.num_features}')\n",
    "\n",
    "# Larger model for bigger dataset\n",
    "model_cfg = TransformerConfig(\n",
    "    num_features=dm.num_features, d_model=256, nhead=8,\n",
    "    num_encoder_layers=6, dim_feedforward=1024, dropout=0.1,\n",
    "    direction_weight=0.3, volatility_weight=0.3,\n",
    ")\n",
    "model = TransformerForecaster(model_cfg)\n",
    "loss_fn = MultiTaskLoss(model_cfg)\n",
    "print(f'Transformer params: {model.count_parameters():,}')\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    epochs=100, learning_rate=1e-4, weight_decay=1e-5,\n",
    "    warmup_steps=3000, patience=10, mixed_precision=True,\n",
    "    checkpoint_dir='outputs/models/transformer_fullmkt',\n",
    ")\n",
    "trainer = Trainer(model, loss_fn, trainer_config, device)\n",
    "\n",
    "start = time.time()\n",
    "trainer.fit(train_loader, val_loader)\n",
    "elapsed = time.time() - start\n",
    "print(f'\\nTransformer training done in {elapsed/60:.1f} min')\n",
    "\n",
    "# Save to Drive\n",
    "for f in Path('outputs/models/transformer_fullmkt').glob('*.pt'):\n",
    "    shutil.copy(f, DRIVE_DIR / 'outputs/models/transformer_fullmkt' / f.name)\n",
    "print('Transformer saved to Drive!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7_tft_header",
   "metadata": {},
   "source": [
    "## Train TFT-small on Full Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8_train_tft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAIN TFT ON FULL MARKET (SMALLER ARCHITECTURE) ===\n",
    "from quant_lab.models.tft.model import TFTForecaster, TFTConfig\n",
    "\n",
    "set_global_seed(42)\n",
    "\n",
    "tft_cfg = TFTConfig(\n",
    "    num_features=dm.num_features, d_model=32, nhead=4,\n",
    "    num_encoder_layers=1, lstm_layers=1, lstm_hidden=32,\n",
    "    grn_hidden=16, dropout=0.3,\n",
    "    direction_weight=0.3, volatility_weight=0.3,\n",
    ")\n",
    "model = TFTForecaster(tft_cfg)\n",
    "loss_cfg = TransformerConfig(num_features=dm.num_features, direction_weight=0.3, volatility_weight=0.3)\n",
    "loss_fn = MultiTaskLoss(loss_cfg)\n",
    "print(f'TFT params: {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    epochs=100, learning_rate=3e-4, weight_decay=1e-3,\n",
    "    warmup_steps=500, patience=15, mixed_precision=True,\n",
    "    checkpoint_dir='outputs/models/tft_fullmkt',\n",
    ")\n",
    "trainer = Trainer(model, loss_fn, trainer_config, device)\n",
    "\n",
    "start = time.time()\n",
    "trainer.fit(train_loader, val_loader)\n",
    "elapsed = time.time() - start\n",
    "print(f'\\nTFT training done in {elapsed/60:.1f} min')\n",
    "\n",
    "# Verify no mode collapse\n",
    "model.eval()\n",
    "x = torch.randn(20, 63, dm.num_features).to(device)\n",
    "with torch.no_grad():\n",
    "    preds = model.predict_returns(x)\n",
    "print(f'Signal std: {preds.std():.6f} (should be >> 0)')\n",
    "\n",
    "# Save to Drive\n",
    "for f in Path('outputs/models/tft_fullmkt').glob('*.pt'):\n",
    "    shutil.copy(f, DRIVE_DIR / 'outputs/models/tft_fullmkt' / f.name)\n",
    "print('TFT saved to Drive!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9_wf_header",
   "metadata": {},
   "source": [
    "## Walk-Forward Validation (Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10_walk_forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === WALK-FORWARD VALIDATION (TRANSFORMER) ===\n",
    "from quant_lab.backtest.walk_forward import WalkForwardEngine, WalkForwardConfig, WindowType\n",
    "from quant_lab.backtest.engine import BacktestConfig\n",
    "from quant_lab.data.datasets import create_flat_datasets\n",
    "\n",
    "prices_df = df[['date', 'ticker', 'adj_close']].copy()\n",
    "\n",
    "wf_config = WalkForwardConfig(\n",
    "    window_type=WindowType.EXPANDING,\n",
    "    train_days=756, val_days=126, test_days=126,\n",
    "    step_days=126, min_train_days=504,\n",
    ")\n",
    "# top_n=20 for broader universe (was 5 for NIFTY 50, 10 for NIFTY 500)\n",
    "backtest_cfg = BacktestConfig(initial_capital=1_000_000, rebalance_frequency=5, top_n=20)\n",
    "\n",
    "def make_transformer_factory():\n",
    "    def factory(split, feature_df, feat_cols):\n",
    "        dm = QuantDataModule(\n",
    "            feature_df, feat_cols, split,\n",
    "            DataModuleConfig(sequence_length=63, target_col='log_return_1d', batch_size=256, num_workers=2),\n",
    "        )\n",
    "        dm.setup()\n",
    "        tl = dm.train_dataloader()\n",
    "        vl = dm.val_dataloader()\n",
    "        if tl is None:\n",
    "            return None, pd.DataFrame()\n",
    "\n",
    "        cfg = TransformerConfig(\n",
    "            num_features=dm.num_features, d_model=256, nhead=8,\n",
    "            num_encoder_layers=6, dim_feedforward=1024, dropout=0.1,\n",
    "            direction_weight=0.3, volatility_weight=0.3,\n",
    "        )\n",
    "        m = TransformerForecaster(cfg)\n",
    "        loss_fn = MultiTaskLoss(cfg)\n",
    "        tc = TrainerConfig(\n",
    "            epochs=30, learning_rate=1e-4, weight_decay=1e-5,\n",
    "            warmup_steps=1000, patience=5, mixed_precision=True,\n",
    "            checkpoint_dir='outputs/walk_forward/transformer_fullmkt',\n",
    "        )\n",
    "        t = Trainer(m, loss_fn, tc, device)\n",
    "        t.fit(tl, vl)\n",
    "\n",
    "        test_loader = dm.test_dataloader()\n",
    "        if test_loader is None:\n",
    "            return m, pd.DataFrame()\n",
    "        m.eval()\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for x, _ in test_loader:\n",
    "                x = x.to(device)\n",
    "                all_preds.append(m.predict_returns(x).cpu().numpy())\n",
    "        test_preds = np.concatenate(all_preds)\n",
    "\n",
    "        datasets = create_flat_datasets(feature_df, feat_cols, split, target_col='log_return_1d')\n",
    "        _, _, meta_test = datasets['test']\n",
    "        meta_test = meta_test.iloc[-len(test_preds):]\n",
    "        signals = meta_test.copy()\n",
    "        signals['signal'] = test_preds\n",
    "        return m, signals\n",
    "    return factory\n",
    "\n",
    "print('Starting full-market Transformer walk-forward (this takes 3-6 hours)...')\n",
    "start = time.time()\n",
    "engine = WalkForwardEngine(wf_config, backtest_cfg)\n",
    "wf_result = engine.run(df, feature_cols, prices_df, make_transformer_factory())\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f'\\nWalk-forward done in {elapsed/60:.1f} min ({len(wf_result.fold_results)} folds)')\n",
    "print(f'\\nAggregate metrics:')\n",
    "for k, v in wf_result.aggregate_metrics.items():\n",
    "    if 'return' in k or 'cagr' in k or 'drawdown' in k:\n",
    "        print(f'  {k:25s}: {v:>10.2%}')\n",
    "    else:\n",
    "        print(f'  {k:25s}: {v:>10.4f}')\n",
    "\n",
    "print(f'\\nPer-fold:')\n",
    "display_cols = ['fold', 'test_start', 'test_end', 'sharpe', 'total_return', 'max_drawdown']\n",
    "available = [c for c in display_cols if c in wf_result.per_fold_metrics.columns]\n",
    "print(wf_result.per_fold_metrics[available].to_string(index=False))\n",
    "\n",
    "# Save to Drive\n",
    "wf_out = DRIVE_DIR / 'outputs/walk_forward/transformer_fullmkt'\n",
    "wf_out.mkdir(parents=True, exist_ok=True)\n",
    "wf_result.per_fold_metrics.to_csv(wf_out / 'per_fold_metrics.csv', index=False)\n",
    "wf_result.aggregate_equity.to_frame('equity').to_parquet(wf_out / 'aggregate_equity.parquet')\n",
    "print(f'\\nResults saved to Drive!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11_summary_header",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('NOTEBOOK F COMPLETE — FULL INDIAN MARKET')\n",
    "print('=' * 60)\n",
    "print(f'\\nUniverse: {df[\"ticker\"].nunique()} stocks (all NSE EQ-series after liquidity filter)')\n",
    "print(f'\\nAll outputs on Drive:')\n",
    "for d in ['outputs/models/transformer_fullmkt', 'outputs/models/tft_fullmkt', 'outputs/walk_forward/transformer_fullmkt']:\n",
    "    p = DRIVE_DIR / d\n",
    "    if p.exists():\n",
    "        for f in sorted(p.glob('*')):\n",
    "            if f.is_file():\n",
    "                print(f'  {f.relative_to(DRIVE_DIR)}: {f.stat().st_size/1e6:.1f} MB')\n",
    "print(f'\\nWalk-forward Sharpe: {wf_result.aggregate_metrics.get(\"sharpe\", \"N/A\"):.4f}')\n",
    "print(f'Walk-forward CAGR: {wf_result.aggregate_metrics.get(\"cagr\", \"N/A\"):.2%}')\n",
    "print('=' * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
