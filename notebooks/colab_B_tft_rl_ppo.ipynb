{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "nbconvert_exporter": "python"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "gpuClass": "premium"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI7wEt5VBG5T"
      },
      "source": [
        "# Notebook B: TFT + RL (PPO) Training\n",
        "**Run on Colab Pro+ H100** | Part 2 of 3 parallel sessions\n",
        "- Trains Temporal Fusion Transformer (TFT)\n",
        "- Trains PPO agent for portfolio allocation"
      ],
      "id": "dI7wEt5VBG5T"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99oKDB2iBG5e",
        "outputId": "f8876942-b901-494b-bca1-bdd400e8f584"
      },
      "source": [
        "# === ENVIRONMENT SETUP ===\n",
        "import subprocess, sys, os\n",
        "\n",
        "if not os.path.exists('/content/quant-lab'):\n",
        "    subprocess.run(['git', 'clone', 'https://github.com/Mohit1053/quant-lab.git', '/content/quant-lab'], check=True)\n",
        "os.chdir('/content/quant-lab')\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-e', '.'], check=True)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "from pathlib import Path\n",
        "DRIVE_DIR = Path('/content/drive/MyDrive/quant_lab')\n",
        "for d in ['data/raw', 'data/cleaned', 'data/features', 'outputs/models/tft', 'outputs/models/rl/ppo', 'outputs/mlruns']:\n",
        "    (DRIVE_DIR / d).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    gpu = torch.cuda.get_device_name(0)\n",
        "    mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {gpu} ({mem:.1f} GB) | BF16: {torch.cuda.is_bf16_supported()}\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "GPU: NVIDIA A100-SXM4-80GB (85.1 GB) | BF16: True\n"
          ]
        }
      ],
      "execution_count": 1,
      "id": "99oKDB2iBG5e"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip uninstall -y numpy pandas scipy scikit-learn\n",
        "!pip install --no-cache-dir numpy==1.26.4 pandas==2.2.2 scipy==1.11.4 scikit-learn==1.4.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "65r2_kw2Uo8A",
        "outputId": "4bead6d2-ed9a-4007-d27c-f6ba1ee9e996"
      },
      "id": "65r2_kw2Uo8A",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: pandas 2.2.2\n",
            "Uninstalling pandas-2.2.2:\n",
            "  Successfully uninstalled pandas-2.2.2\n",
            "Found existing installation: scipy 1.16.3\n",
            "Uninstalling scipy-1.16.3:\n",
            "  Successfully uninstalled scipy-1.16.3\n",
            "Found existing installation: scikit-learn 1.6.1\n",
            "Uninstalling scikit-learn-1.6.1:\n",
            "  Successfully uninstalled scikit-learn-1.6.1\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==2.2.2\n",
            "  Downloading pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting scipy==1.11.4\n",
            "  Downloading scipy-1.11.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.4.2\n",
            "  Downloading scikit_learn-1.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2025.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.4.2) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.4.2) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m354.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m301.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.8/35.8 MB\u001b[0m \u001b[31m231.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m185.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, pandas, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "libpysal 4.14.1 requires scipy>=1.12.0, but you have scipy 1.11.4 which is incompatible.\n",
            "opencv-python-headless 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\n",
            "inequality 1.1.2 requires scipy>=1.12, but you have scipy 1.11.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\n",
            "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "hdbscan 0.8.41 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "access 1.1.10.post3 requires scipy>=1.14.1, but you have scipy 1.11.4 which is incompatible.\n",
            "esda 2.8.1 requires scipy>=1.12, but you have scipy 1.11.4 which is incompatible.\n",
            "mapclassify 2.10.0 requires scipy>=1.12, but you have scipy 1.11.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\n",
            "spopt 0.7.0 requires scipy>=1.12.0, but you have scipy 1.11.4 which is incompatible.\n",
            "opencv-contrib-python 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "umap-learn 0.5.11 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "giddy 2.3.8 requires scipy>=1.12, but you have scipy 1.11.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 pandas-2.2.2 scikit-learn-1.4.2 scipy-1.11.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "2360eb80adfc4dbfa3892c7c4eeeca67"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zahy-u6ZBG5k",
        "outputId": "f64af7a7-5a9e-4e45-c6b7-9fbd6a258201"
      },
      "source": [
        "# === LOAD DATA FROM DRIVE (cached by Notebook A) ===\n",
        "import shutil, time\n",
        "\n",
        "drive_features = DRIVE_DIR / 'data/features/nifty50_features.parquet'\n",
        "local_features = Path('data/features/nifty50_features.parquet')\n",
        "\n",
        "# Wait for Notebook A to cache data (max 10 min)\n",
        "if not drive_features.exists():\n",
        "    print(\"Waiting for Notebook A to cache data to Drive...\")\n",
        "    for i in range(60):  # 10 min max\n",
        "        if drive_features.exists():\n",
        "            break\n",
        "        time.sleep(10)\n",
        "        if i % 6 == 0:\n",
        "            print(f\"  Still waiting... ({i*10}s)\")\n",
        "    else:\n",
        "        print(\"Timeout! Downloading data ourselves...\")\n",
        "        subprocess.run([sys.executable, 'scripts/ingest_data.py'], check=True)\n",
        "        subprocess.run([sys.executable, 'scripts/compute_features.py'], check=True)\n",
        "\n",
        "if drive_features.exists():\n",
        "    Path('data/features').mkdir(parents=True, exist_ok=True)\n",
        "    Path('data/cleaned').mkdir(parents=True, exist_ok=True)\n",
        "    shutil.copy(drive_features, local_features)\n",
        "    if (DRIVE_DIR / 'data/cleaned/nifty50_cleaned.parquet').exists():\n",
        "        shutil.copy(DRIVE_DIR / 'data/cleaned/nifty50_cleaned.parquet', 'data/cleaned/nifty50_cleaned.parquet')\n",
        "    print(\"Data loaded from Drive cache!\")\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_parquet(local_features)\n",
        "print(f\"Features: {df.shape[0]} rows, {df['ticker'].nunique()} tickers\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded from Drive cache!\n",
            "Features: 177187 rows, 49 tickers\n"
          ]
        }
      ],
      "execution_count": 6,
      "id": "zahy-u6ZBG5k"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnBRHqFgBG5m"
      },
      "source": [
        "## Temporal Fusion Transformer (TFT)\n",
        "GRN blocks + Variable Selection + LSTM encoder + interpretable multi-head attention\n",
        "- H100 config: d_model=256, 4 heads, 2 layers, batch_size=128"
      ],
      "id": "lnBRHqFgBG5m"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmhgCL92BG5o",
        "outputId": "62078083-3980-401d-d3e4-86fd4c054fc7"
      },
      "source": [
        "# === TFT TRAINING (H100 optimized) ===\n",
        "import time\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from quant_lab.utils.seed import set_global_seed\n",
        "from quant_lab.utils.device import get_device\n",
        "from quant_lab.data.datasets import TemporalSplit\n",
        "from quant_lab.data.datamodule import QuantDataModule, DataModuleConfig\n",
        "from quant_lab.data.storage.parquet_store import ParquetStore\n",
        "from quant_lab.features.engine import FeatureEngine\n",
        "from quant_lab.models.tft.model import TFTForecaster, TFTConfig\n",
        "from quant_lab.models.transformer.model import MultiTaskLoss, TransformerConfig\n",
        "from quant_lab.training.trainer import Trainer, TrainerConfig\n",
        "\n",
        "# -----------------------------\n",
        "# Setup\n",
        "# -----------------------------\n",
        "set_global_seed(42)\n",
        "device = get_device()\n",
        "\n",
        "# Load features\n",
        "store = ParquetStore(base_dir='data/features')\n",
        "feature_df = store.load('nifty50_features')\n",
        "\n",
        "engine = FeatureEngine(\n",
        "    enabled_features=['log_returns', 'realized_volatility', 'momentum', 'max_drawdown'],\n",
        "    windows={'short': [1, 5], 'medium': [21], 'long': [63]},\n",
        ")\n",
        "feature_cols = engine.get_feature_columns(feature_df)\n",
        "\n",
        "split = TemporalSplit(train_end='2021-12-31', val_end='2023-06-30')\n",
        "dm = QuantDataModule(\n",
        "    feature_df, feature_cols, split,\n",
        "    DataModuleConfig(sequence_length=63, target_col='log_return_1d', batch_size=128, num_workers=2),\n",
        ")\n",
        "dm.setup()\n",
        "train_loader = dm.train_dataloader()\n",
        "val_loader = dm.val_dataloader()\n",
        "\n",
        "# -----------------------------\n",
        "# TFT Model\n",
        "# -----------------------------\n",
        "tft_cfg = TFTConfig(\n",
        "    num_features=dm.num_features,\n",
        "    d_model=256,\n",
        "    nhead=4,\n",
        "    num_encoder_layers=2,\n",
        "    lstm_layers=1,\n",
        "    lstm_hidden=256,\n",
        "    dropout=0.1,\n",
        "    grn_hidden=128,\n",
        ")\n",
        "\n",
        "# Correct model class\n",
        "model = TFTForecaster(tft_cfg)\n",
        "\n",
        "# Loss function\n",
        "loss_cfg = TransformerConfig(\n",
        "    num_features=dm.num_features,\n",
        "    d_model=256,\n",
        "    distribution_type='gaussian',\n",
        "    direction_num_classes=3,\n",
        "    direction_threshold=0.005,\n",
        "    volatility_enabled=True,\n",
        "    distribution_weight=1.0,\n",
        "    direction_weight=0.3,\n",
        "    volatility_weight=0.3,\n",
        ")\n",
        "loss_fn = MultiTaskLoss(loss_cfg)\n",
        "\n",
        "print(f\"TFT parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Trainer\n",
        "# -----------------------------\n",
        "trainer_config = TrainerConfig(\n",
        "    epochs=100,\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_grad_norm=1.0,\n",
        "    patience=15,\n",
        "    mixed_precision=True,\n",
        "    checkpoint_dir='outputs/models/tft',\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, loss_fn=loss_fn, config=trainer_config, device=device)\n",
        "\n",
        "# -----------------------------\n",
        "# Training\n",
        "# -----------------------------\n",
        "start = time.time()\n",
        "history = trainer.fit(train_loader, val_loader)\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"\\nTFT training done in {elapsed/60:.1f} min\")\n",
        "print(f\"Final train loss: {history['train_loss'][-1]:.6f}\")\n",
        "if history['val_loss']:\n",
        "    print(f\"Best val loss: {min(history['val_loss']):.6f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Save model to Drive\n",
        "# -----------------------------\n",
        "model_path = Path('outputs/models/tft/final_model.pt')\n",
        "model.save(model_path)\n",
        "\n",
        "drive_tft_dir = DRIVE_DIR / 'outputs/models/tft'\n",
        "drive_tft_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for f in Path('outputs/models/tft').glob('*'):\n",
        "    shutil.copy(f, drive_tft_dir / f.name)\n",
        "\n",
        "print(\"TFT model saved to Drive!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-19 10:27:09 [info     ] using_gpu                      memory_gb=79.3 name='NVIDIA A100-SXM4-80GB'\n",
            "2026-02-19 10:27:09 [info     ] parquet_loaded                 cols=23 path=data/features/nifty50_features.parquet rows=177187\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2750 valid_samples=2499\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2962 valid_samples=2711\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=1018 valid_samples=767\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:10 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=1050 valid_samples=799\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=2961 valid_samples=2710\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:11 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=370 valid_samples=307\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [debug    ] dataset_created                num_features=15 sequence_length=63 total_rows=368 valid_samples=305\n",
            "2026-02-19 10:27:12 [info     ] split_train                    num_tickers=49 total_samples=128726\n",
            "2026-02-19 10:27:12 [info     ] split_val                      num_tickers=49 total_samples=15043\n",
            "2026-02-19 10:27:12 [info     ] split_test                     num_tickers=49 total_samples=14945\n",
            "2026-02-19 10:27:12 [info     ] datamodule_setup               num_features=15 sequence_length=63 test_samples=14945 train_samples=128726 val_samples=15043\n",
            "TFT parameters: 35,606,293\n",
            "2026-02-19 10:27:12 [info     ] training_start                 amp_dtype=torch.bfloat16 device=cuda epochs=100 parameters=35606293 total_steps=100500\n",
            "2026-02-19 10:27:15 [debug    ] train_step                     loss=-1.767570 lr=1.00e-04 step=50\n",
            "2026-02-19 10:27:18 [debug    ] train_step                     loss=2.708782 lr=2.00e-04 step=100\n",
            "2026-02-19 10:27:21 [debug    ] train_step                     loss=-1.921634 lr=3.00e-04 step=150\n",
            "2026-02-19 10:27:24 [debug    ] train_step                     loss=-2.175508 lr=4.00e-04 step=200\n",
            "2026-02-19 10:27:27 [debug    ] train_step                     loss=-0.434732 lr=5.00e-04 step=250\n",
            "2026-02-19 10:27:30 [debug    ] train_step                     loss=-2.316746 lr=6.00e-04 step=300\n",
            "2026-02-19 10:27:33 [debug    ] train_step                     loss=-2.442341 lr=7.00e-04 step=350\n",
            "2026-02-19 10:27:36 [debug    ] train_step                     loss=-0.636105 lr=8.00e-04 step=400\n",
            "2026-02-19 10:27:39 [debug    ] train_step                     loss=-2.590044 lr=9.00e-04 step=450\n",
            "2026-02-19 10:27:42 [debug    ] train_step                     loss=-2.674776 lr=1.00e-03 step=500\n",
            "2026-02-19 10:27:45 [debug    ] train_step                     loss=-1.223294 lr=1.00e-03 step=550\n",
            "2026-02-19 10:27:48 [debug    ] train_step                     loss=-2.826777 lr=1.00e-03 step=600\n",
            "2026-02-19 10:27:51 [debug    ] train_step                     loss=-3.005642 lr=1.00e-03 step=650\n",
            "2026-02-19 10:27:54 [debug    ] train_step                     loss=-1.608112 lr=1.00e-03 step=700\n",
            "2026-02-19 10:27:57 [debug    ] train_step                     loss=-2.868650 lr=1.00e-03 step=750\n",
            "2026-02-19 10:28:00 [debug    ] train_step                     loss=-3.095241 lr=1.00e-03 step=800\n",
            "2026-02-19 10:28:03 [debug    ] train_step                     loss=-1.936549 lr=1.00e-03 step=850\n",
            "2026-02-19 10:28:06 [debug    ] train_step                     loss=-2.556808 lr=1.00e-03 step=900\n",
            "2026-02-19 10:28:09 [debug    ] train_step                     loss=-2.918407 lr=1.00e-03 step=950\n",
            "2026-02-19 10:28:12 [debug    ] train_step                     loss=-2.542485 lr=1.00e-03 step=1000\n",
            "2026-02-19 10:28:15 [info     ] epoch_complete                 epoch=1 lr=1.00e-03 train_loss=-1.997769 val_loss=-2.458531\n",
            "2026-02-19 10:28:16 [info     ] checkpoint_saved               epoch=0 path=best.pt score=-2.4585314656023756\n",
            "2026-02-19 10:28:19 [debug    ] train_step                     loss=-3.019376 lr=1.00e-03 step=1050\n",
            "2026-02-19 10:28:22 [debug    ] train_step                     loss=-3.116757 lr=1.00e-03 step=1100\n",
            "2026-02-19 10:28:25 [debug    ] train_step                     loss=-2.249385 lr=1.00e-03 step=1150\n",
            "2026-02-19 10:28:28 [debug    ] train_step                     loss=-3.035902 lr=1.00e-03 step=1200\n",
            "2026-02-19 10:28:31 [debug    ] train_step                     loss=-2.929845 lr=1.00e-03 step=1250\n",
            "2026-02-19 10:28:34 [debug    ] train_step                     loss=-2.509897 lr=1.00e-03 step=1300\n",
            "2026-02-19 10:28:37 [debug    ] train_step                     loss=-2.911617 lr=1.00e-03 step=1350\n",
            "2026-02-19 10:28:40 [debug    ] train_step                     loss=-3.049049 lr=1.00e-03 step=1400\n",
            "2026-02-19 10:28:43 [debug    ] train_step                     loss=-3.067093 lr=1.00e-03 step=1450\n",
            "2026-02-19 10:28:46 [debug    ] train_step                     loss=-2.491186 lr=1.00e-03 step=1500\n",
            "2026-02-19 10:28:49 [debug    ] train_step                     loss=-2.622457 lr=1.00e-03 step=1550\n",
            "2026-02-19 10:28:52 [debug    ] train_step                     loss=-3.120416 lr=1.00e-03 step=1600\n",
            "2026-02-19 10:28:55 [debug    ] train_step                     loss=-2.784847 lr=1.00e-03 step=1650\n",
            "2026-02-19 10:28:58 [debug    ] train_step                     loss=-3.022842 lr=1.00e-03 step=1700\n",
            "2026-02-19 10:29:01 [debug    ] train_step                     loss=-3.111526 lr=1.00e-03 step=1750\n",
            "2026-02-19 10:29:04 [debug    ] train_step                     loss=-2.766020 lr=1.00e-03 step=1800\n",
            "2026-02-19 10:29:07 [debug    ] train_step                     loss=-2.930114 lr=1.00e-03 step=1850\n",
            "2026-02-19 10:29:10 [debug    ] train_step                     loss=-3.135603 lr=1.00e-03 step=1900\n",
            "2026-02-19 10:29:13 [debug    ] train_step                     loss=-2.801896 lr=9.99e-04 step=1950\n",
            "2026-02-19 10:29:16 [debug    ] train_step                     loss=-2.946899 lr=9.99e-04 step=2000\n",
            "2026-02-19 10:29:18 [info     ] epoch_complete                 epoch=2 lr=9.99e-04 train_loss=-2.879245 val_loss=-3.091215\n",
            "2026-02-19 10:29:19 [info     ] checkpoint_saved               epoch=1 path=best.pt score=-3.091215370065075\n",
            "2026-02-19 10:29:23 [debug    ] train_step                     loss=-3.117113 lr=9.99e-04 step=2050\n",
            "2026-02-19 10:29:26 [debug    ] train_step                     loss=-2.614023 lr=9.99e-04 step=2100\n",
            "2026-02-19 10:29:29 [debug    ] train_step                     loss=-2.984029 lr=9.99e-04 step=2150\n",
            "2026-02-19 10:29:32 [debug    ] train_step                     loss=-2.868653 lr=9.99e-04 step=2200\n",
            "2026-02-19 10:29:35 [debug    ] train_step                     loss=-2.999263 lr=9.99e-04 step=2250\n",
            "2026-02-19 10:29:37 [debug    ] train_step                     loss=-2.836718 lr=9.99e-04 step=2300\n",
            "2026-02-19 10:29:40 [debug    ] train_step                     loss=-3.047703 lr=9.99e-04 step=2350\n",
            "2026-02-19 10:29:43 [debug    ] train_step                     loss=-2.912757 lr=9.99e-04 step=2400\n",
            "2026-02-19 10:29:46 [debug    ] train_step                     loss=-2.854673 lr=9.99e-04 step=2450\n",
            "2026-02-19 10:29:49 [debug    ] train_step                     loss=-2.847098 lr=9.99e-04 step=2500\n",
            "2026-02-19 10:29:52 [debug    ] train_step                     loss=-2.914241 lr=9.99e-04 step=2550\n",
            "2026-02-19 10:29:55 [debug    ] train_step                     loss=-2.995125 lr=9.99e-04 step=2600\n",
            "2026-02-19 10:29:58 [debug    ] train_step                     loss=-2.894315 lr=9.99e-04 step=2650\n",
            "2026-02-19 10:30:01 [debug    ] train_step                     loss=-2.875970 lr=9.99e-04 step=2700\n",
            "2026-02-19 10:30:04 [debug    ] train_step                     loss=-2.988984 lr=9.99e-04 step=2750\n",
            "2026-02-19 10:30:07 [debug    ] train_step                     loss=-2.039725 lr=9.99e-04 step=2800\n",
            "2026-02-19 10:30:10 [debug    ] train_step                     loss=-3.235491 lr=9.99e-04 step=2850\n",
            "2026-02-19 10:30:13 [debug    ] train_step                     loss=-2.718957 lr=9.99e-04 step=2900\n",
            "2026-02-19 10:30:16 [debug    ] train_step                     loss=-2.469790 lr=9.99e-04 step=2950\n",
            "2026-02-19 10:30:19 [debug    ] train_step                     loss=-2.585293 lr=9.98e-04 step=3000\n",
            "2026-02-19 10:30:22 [info     ] epoch_complete                 epoch=3 lr=9.98e-04 train_loss=-2.873523 val_loss=-2.654234\n",
            "2026-02-19 10:30:25 [debug    ] train_step                     loss=-2.983162 lr=9.98e-04 step=3050\n",
            "2026-02-19 10:30:28 [debug    ] train_step                     loss=-2.594203 lr=9.98e-04 step=3100\n",
            "2026-02-19 10:30:31 [debug    ] train_step                     loss=-2.699898 lr=9.98e-04 step=3150\n",
            "2026-02-19 10:30:34 [debug    ] train_step                     loss=-2.823607 lr=9.98e-04 step=3200\n",
            "2026-02-19 10:30:37 [debug    ] train_step                     loss=-2.860796 lr=9.98e-04 step=3250\n",
            "2026-02-19 10:30:40 [debug    ] train_step                     loss=-2.871651 lr=9.98e-04 step=3300\n",
            "2026-02-19 10:30:43 [debug    ] train_step                     loss=-2.906157 lr=9.98e-04 step=3350\n",
            "2026-02-19 10:30:46 [debug    ] train_step                     loss=-2.968720 lr=9.98e-04 step=3400\n",
            "2026-02-19 10:30:49 [debug    ] train_step                     loss=-2.932015 lr=9.98e-04 step=3450\n",
            "2026-02-19 10:30:52 [debug    ] train_step                     loss=-3.073322 lr=9.98e-04 step=3500\n",
            "2026-02-19 10:30:54 [debug    ] train_step                     loss=-3.000986 lr=9.98e-04 step=3550\n",
            "2026-02-19 10:30:57 [debug    ] train_step                     loss=-3.062623 lr=9.98e-04 step=3600\n",
            "2026-02-19 10:31:00 [debug    ] train_step                     loss=-2.761440 lr=9.98e-04 step=3650\n",
            "2026-02-19 10:31:03 [debug    ] train_step                     loss=-2.991072 lr=9.98e-04 step=3700\n",
            "2026-02-19 10:31:06 [debug    ] train_step                     loss=-2.647068 lr=9.97e-04 step=3750\n",
            "2026-02-19 10:31:09 [debug    ] train_step                     loss=-3.027024 lr=9.97e-04 step=3800\n",
            "2026-02-19 10:31:12 [debug    ] train_step                     loss=-3.081365 lr=9.97e-04 step=3850\n",
            "2026-02-19 10:31:15 [debug    ] train_step                     loss=-3.087707 lr=9.97e-04 step=3900\n",
            "2026-02-19 10:31:18 [debug    ] train_step                     loss=-3.194514 lr=9.97e-04 step=3950\n",
            "2026-02-19 10:31:21 [debug    ] train_step                     loss=-2.999691 lr=9.97e-04 step=4000\n",
            "2026-02-19 10:31:24 [info     ] epoch_complete                 epoch=4 lr=9.97e-04 train_loss=-2.955614 val_loss=-3.007072\n",
            "2026-02-19 10:31:27 [debug    ] train_step                     loss=-3.209371 lr=9.97e-04 step=4050\n",
            "2026-02-19 10:31:30 [debug    ] train_step                     loss=-3.054554 lr=9.97e-04 step=4100\n",
            "2026-02-19 10:31:33 [debug    ] train_step                     loss=-3.169559 lr=9.97e-04 step=4150\n",
            "2026-02-19 10:31:36 [debug    ] train_step                     loss=-3.155581 lr=9.97e-04 step=4200\n",
            "2026-02-19 10:31:39 [debug    ] train_step                     loss=-2.991426 lr=9.97e-04 step=4250\n",
            "2026-02-19 10:31:42 [debug    ] train_step                     loss=-3.030417 lr=9.96e-04 step=4300\n",
            "2026-02-19 10:31:45 [debug    ] train_step                     loss=-2.892728 lr=9.96e-04 step=4350\n",
            "2026-02-19 10:31:48 [debug    ] train_step                     loss=-2.986815 lr=9.96e-04 step=4400\n",
            "2026-02-19 10:31:51 [debug    ] train_step                     loss=-2.969287 lr=9.96e-04 step=4450\n",
            "2026-02-19 10:31:54 [debug    ] train_step                     loss=-2.970104 lr=9.96e-04 step=4500\n",
            "2026-02-19 10:31:57 [debug    ] train_step                     loss=-3.177966 lr=9.96e-04 step=4550\n",
            "2026-02-19 10:32:00 [debug    ] train_step                     loss=-3.068825 lr=9.96e-04 step=4600\n",
            "2026-02-19 10:32:03 [debug    ] train_step                     loss=-2.943328 lr=9.96e-04 step=4650\n",
            "2026-02-19 10:32:05 [debug    ] train_step                     loss=-2.935624 lr=9.96e-04 step=4700\n",
            "2026-02-19 10:32:08 [debug    ] train_step                     loss=-3.125812 lr=9.96e-04 step=4750\n",
            "2026-02-19 10:32:11 [debug    ] train_step                     loss=-3.161109 lr=9.95e-04 step=4800\n",
            "2026-02-19 10:32:14 [debug    ] train_step                     loss=-3.058029 lr=9.95e-04 step=4850\n",
            "2026-02-19 10:32:17 [debug    ] train_step                     loss=-3.188583 lr=9.95e-04 step=4900\n",
            "2026-02-19 10:32:20 [debug    ] train_step                     loss=-2.570853 lr=9.95e-04 step=4950\n",
            "2026-02-19 10:32:23 [debug    ] train_step                     loss=-3.218189 lr=9.95e-04 step=5000\n",
            "2026-02-19 10:32:27 [info     ] epoch_complete                 epoch=5 lr=9.95e-04 train_loss=-2.956201 val_loss=-3.051713\n",
            "2026-02-19 10:32:29 [debug    ] train_step                     loss=-2.859134 lr=9.95e-04 step=5050\n",
            "2026-02-19 10:32:32 [debug    ] train_step                     loss=-2.900159 lr=9.95e-04 step=5100\n",
            "2026-02-19 10:32:35 [debug    ] train_step                     loss=-3.132879 lr=9.95e-04 step=5150\n",
            "2026-02-19 10:32:38 [debug    ] train_step                     loss=-2.964704 lr=9.95e-04 step=5200\n",
            "2026-02-19 10:32:41 [debug    ] train_step                     loss=-3.060060 lr=9.94e-04 step=5250\n",
            "2026-02-19 10:32:44 [debug    ] train_step                     loss=-3.193198 lr=9.94e-04 step=5300\n",
            "2026-02-19 10:32:47 [debug    ] train_step                     loss=-2.864846 lr=9.94e-04 step=5350\n",
            "2026-02-19 10:32:50 [debug    ] train_step                     loss=-3.006674 lr=9.94e-04 step=5400\n",
            "2026-02-19 10:32:53 [debug    ] train_step                     loss=-2.955676 lr=9.94e-04 step=5450\n",
            "2026-02-19 10:32:56 [debug    ] train_step                     loss=-2.766720 lr=9.94e-04 step=5500\n",
            "2026-02-19 10:32:59 [debug    ] train_step                     loss=-3.080259 lr=9.94e-04 step=5550\n",
            "2026-02-19 10:33:02 [debug    ] train_step                     loss=-3.115071 lr=9.94e-04 step=5600\n",
            "2026-02-19 10:33:05 [debug    ] train_step                     loss=-3.042741 lr=9.94e-04 step=5650\n",
            "2026-02-19 10:33:08 [debug    ] train_step                     loss=-2.933771 lr=9.93e-04 step=5700\n",
            "2026-02-19 10:33:10 [debug    ] train_step                     loss=-3.178210 lr=9.93e-04 step=5750\n",
            "2026-02-19 10:33:13 [debug    ] train_step                     loss=-3.053954 lr=9.93e-04 step=5800\n",
            "2026-02-19 10:33:16 [debug    ] train_step                     loss=-3.115969 lr=9.93e-04 step=5850\n",
            "2026-02-19 10:33:19 [debug    ] train_step                     loss=-2.949386 lr=9.93e-04 step=5900\n",
            "2026-02-19 10:33:22 [debug    ] train_step                     loss=-2.962495 lr=9.93e-04 step=5950\n",
            "2026-02-19 10:33:25 [debug    ] train_step                     loss=-3.011371 lr=9.93e-04 step=6000\n",
            "2026-02-19 10:33:29 [info     ] epoch_complete                 epoch=6 lr=9.93e-04 train_loss=-2.987354 val_loss=-3.202992\n",
            "2026-02-19 10:33:30 [info     ] checkpoint_saved               epoch=5 path=best.pt score=-3.202991734116764\n",
            "2026-02-19 10:33:32 [debug    ] train_step                     loss=-3.175844 lr=9.92e-04 step=6050\n",
            "2026-02-19 10:33:35 [debug    ] train_step                     loss=-3.157298 lr=9.92e-04 step=6100\n",
            "2026-02-19 10:33:38 [debug    ] train_step                     loss=-2.309185 lr=9.92e-04 step=6150\n",
            "2026-02-19 10:33:41 [debug    ] train_step                     loss=-2.923240 lr=9.92e-04 step=6200\n",
            "2026-02-19 10:33:44 [debug    ] train_step                     loss=-2.983328 lr=9.92e-04 step=6250\n",
            "2026-02-19 10:33:47 [debug    ] train_step                     loss=-3.005690 lr=9.92e-04 step=6300\n",
            "2026-02-19 10:33:50 [debug    ] train_step                     loss=-2.912919 lr=9.92e-04 step=6350\n",
            "2026-02-19 10:33:53 [debug    ] train_step                     loss=-2.818096 lr=9.92e-04 step=6400\n",
            "2026-02-19 10:33:56 [debug    ] train_step                     loss=-2.828414 lr=9.91e-04 step=6450\n",
            "2026-02-19 10:33:59 [debug    ] train_step                     loss=-2.571159 lr=9.91e-04 step=6500\n",
            "2026-02-19 10:34:02 [debug    ] train_step                     loss=-3.179529 lr=9.91e-04 step=6550\n",
            "2026-02-19 10:34:05 [debug    ] train_step                     loss=-3.015862 lr=9.91e-04 step=6600\n",
            "2026-02-19 10:34:08 [debug    ] train_step                     loss=-2.937986 lr=9.91e-04 step=6650\n",
            "2026-02-19 10:34:11 [debug    ] train_step                     loss=-3.101965 lr=9.91e-04 step=6700\n",
            "2026-02-19 10:34:14 [debug    ] train_step                     loss=-3.150066 lr=9.90e-04 step=6750\n",
            "2026-02-19 10:34:17 [debug    ] train_step                     loss=-3.074657 lr=9.90e-04 step=6800\n",
            "2026-02-19 10:34:19 [debug    ] train_step                     loss=-3.157834 lr=9.90e-04 step=6850\n",
            "2026-02-19 10:34:22 [debug    ] train_step                     loss=-3.013319 lr=9.90e-04 step=6900\n",
            "2026-02-19 10:34:25 [debug    ] train_step                     loss=-3.136768 lr=9.90e-04 step=6950\n",
            "2026-02-19 10:34:28 [debug    ] train_step                     loss=-2.922559 lr=9.90e-04 step=7000\n",
            "2026-02-19 10:34:33 [info     ] epoch_complete                 epoch=7 lr=9.90e-04 train_loss=-2.986458 val_loss=-3.151654\n",
            "2026-02-19 10:34:35 [debug    ] train_step                     loss=-3.008365 lr=9.90e-04 step=7050\n",
            "2026-02-19 10:34:37 [debug    ] train_step                     loss=-3.002513 lr=9.89e-04 step=7100\n",
            "2026-02-19 10:34:40 [debug    ] train_step                     loss=-2.668855 lr=9.89e-04 step=7150\n",
            "2026-02-19 10:34:43 [debug    ] train_step                     loss=-3.104411 lr=9.89e-04 step=7200\n",
            "2026-02-19 10:34:46 [debug    ] train_step                     loss=-3.063886 lr=9.89e-04 step=7250\n",
            "2026-02-19 10:34:49 [debug    ] train_step                     loss=-2.815728 lr=9.89e-04 step=7300\n",
            "2026-02-19 10:34:52 [debug    ] train_step                     loss=-3.016818 lr=9.89e-04 step=7350\n",
            "2026-02-19 10:34:55 [debug    ] train_step                     loss=-2.954071 lr=9.88e-04 step=7400\n",
            "2026-02-19 10:34:58 [debug    ] train_step                     loss=-3.066618 lr=9.88e-04 step=7450\n",
            "2026-02-19 10:35:01 [debug    ] train_step                     loss=-2.896968 lr=9.88e-04 step=7500\n",
            "2026-02-19 10:35:04 [debug    ] train_step                     loss=-3.199308 lr=9.88e-04 step=7550\n",
            "2026-02-19 10:35:07 [debug    ] train_step                     loss=-2.941650 lr=9.88e-04 step=7600\n",
            "2026-02-19 10:35:10 [debug    ] train_step                     loss=-2.656132 lr=9.88e-04 step=7650\n",
            "2026-02-19 10:35:13 [debug    ] train_step                     loss=-3.022856 lr=9.87e-04 step=7700\n",
            "2026-02-19 10:35:16 [debug    ] train_step                     loss=-2.924781 lr=9.87e-04 step=7750\n",
            "2026-02-19 10:35:19 [debug    ] train_step                     loss=-3.054045 lr=9.87e-04 step=7800\n",
            "2026-02-19 10:35:21 [debug    ] train_step                     loss=-3.125390 lr=9.87e-04 step=7850\n",
            "2026-02-19 10:35:24 [debug    ] train_step                     loss=-2.822446 lr=9.87e-04 step=7900\n",
            "2026-02-19 10:35:27 [debug    ] train_step                     loss=-2.973605 lr=9.87e-04 step=7950\n",
            "2026-02-19 10:35:30 [debug    ] train_step                     loss=-3.105270 lr=9.86e-04 step=8000\n",
            "2026-02-19 10:35:35 [info     ] epoch_complete                 epoch=8 lr=9.86e-04 train_loss=-2.981419 val_loss=-2.824689\n",
            "2026-02-19 10:35:37 [debug    ] train_step                     loss=-2.852830 lr=9.86e-04 step=8050\n",
            "2026-02-19 10:35:39 [debug    ] train_step                     loss=-2.912971 lr=9.86e-04 step=8100\n",
            "2026-02-19 10:35:42 [debug    ] train_step                     loss=-3.138097 lr=9.86e-04 step=8150\n",
            "2026-02-19 10:35:45 [debug    ] train_step                     loss=-3.014159 lr=9.86e-04 step=8200\n",
            "2026-02-19 10:35:48 [debug    ] train_step                     loss=-3.097335 lr=9.85e-04 step=8250\n",
            "2026-02-19 10:35:51 [debug    ] train_step                     loss=-3.127932 lr=9.85e-04 step=8300\n",
            "2026-02-19 10:35:54 [debug    ] train_step                     loss=-2.917484 lr=9.85e-04 step=8350\n",
            "2026-02-19 10:35:57 [debug    ] train_step                     loss=-2.988870 lr=9.85e-04 step=8400\n",
            "2026-02-19 10:36:00 [debug    ] train_step                     loss=-3.082391 lr=9.85e-04 step=8450\n",
            "2026-02-19 10:36:03 [debug    ] train_step                     loss=-3.293262 lr=9.84e-04 step=8500\n",
            "2026-02-19 10:36:06 [debug    ] train_step                     loss=-3.055209 lr=9.84e-04 step=8550\n",
            "2026-02-19 10:36:09 [debug    ] train_step                     loss=-2.938350 lr=9.84e-04 step=8600\n",
            "2026-02-19 10:36:12 [debug    ] train_step                     loss=-2.959313 lr=9.84e-04 step=8650\n",
            "2026-02-19 10:36:15 [debug    ] train_step                     loss=-3.143005 lr=9.84e-04 step=8700\n",
            "2026-02-19 10:36:18 [debug    ] train_step                     loss=-3.227779 lr=9.83e-04 step=8750\n",
            "2026-02-19 10:36:21 [debug    ] train_step                     loss=-3.116872 lr=9.83e-04 step=8800\n",
            "2026-02-19 10:36:24 [debug    ] train_step                     loss=-3.053106 lr=9.83e-04 step=8850\n",
            "2026-02-19 10:36:27 [debug    ] train_step                     loss=-3.213593 lr=9.83e-04 step=8900\n",
            "2026-02-19 10:36:30 [debug    ] train_step                     loss=-3.051079 lr=9.83e-04 step=8950\n",
            "2026-02-19 10:36:33 [debug    ] train_step                     loss=-2.978404 lr=9.82e-04 step=9000\n",
            "2026-02-19 10:36:37 [info     ] epoch_complete                 epoch=9 lr=9.82e-04 train_loss=-2.974277 val_loss=-3.104205\n",
            "2026-02-19 10:36:39 [debug    ] train_step                     loss=-3.094661 lr=9.82e-04 step=9050\n",
            "2026-02-19 10:36:42 [debug    ] train_step                     loss=-2.933869 lr=9.82e-04 step=9100\n",
            "2026-02-19 10:36:45 [debug    ] train_step                     loss=-3.167020 lr=9.82e-04 step=9150\n",
            "2026-02-19 10:36:48 [debug    ] train_step                     loss=-2.950550 lr=9.82e-04 step=9200\n",
            "2026-02-19 10:36:50 [debug    ] train_step                     loss=-2.841170 lr=9.81e-04 step=9250\n",
            "2026-02-19 10:36:53 [debug    ] train_step                     loss=-2.893813 lr=9.81e-04 step=9300\n",
            "2026-02-19 10:36:56 [debug    ] train_step                     loss=-2.797129 lr=9.81e-04 step=9350\n",
            "2026-02-19 10:36:59 [debug    ] train_step                     loss=-3.045950 lr=9.81e-04 step=9400\n",
            "2026-02-19 10:37:02 [debug    ] train_step                     loss=-3.114206 lr=9.81e-04 step=9450\n",
            "2026-02-19 10:37:05 [debug    ] train_step                     loss=-3.061701 lr=9.80e-04 step=9500\n",
            "2026-02-19 10:37:08 [debug    ] train_step                     loss=-3.352728 lr=9.80e-04 step=9550\n",
            "2026-02-19 10:37:11 [debug    ] train_step                     loss=-3.158646 lr=9.80e-04 step=9600\n",
            "2026-02-19 10:37:14 [debug    ] train_step                     loss=-2.882777 lr=9.80e-04 step=9650\n",
            "2026-02-19 10:37:17 [debug    ] train_step                     loss=-2.978230 lr=9.79e-04 step=9700\n",
            "2026-02-19 10:37:20 [debug    ] train_step                     loss=-3.063975 lr=9.79e-04 step=9750\n",
            "2026-02-19 10:37:23 [debug    ] train_step                     loss=-3.090653 lr=9.79e-04 step=9800\n",
            "2026-02-19 10:37:26 [debug    ] train_step                     loss=-3.039826 lr=9.79e-04 step=9850\n",
            "2026-02-19 10:37:29 [debug    ] train_step                     loss=-3.022321 lr=9.79e-04 step=9900\n",
            "2026-02-19 10:37:31 [debug    ] train_step                     loss=-3.103638 lr=9.78e-04 step=9950\n",
            "2026-02-19 10:37:34 [debug    ] train_step                     loss=-2.910802 lr=9.78e-04 step=10000\n",
            "2026-02-19 10:37:37 [debug    ] train_step                     loss=-3.032330 lr=9.78e-04 step=10050\n",
            "2026-02-19 10:37:39 [info     ] epoch_complete                 epoch=10 lr=9.78e-04 train_loss=-2.999443 val_loss=-3.146981\n",
            "2026-02-19 10:37:44 [debug    ] train_step                     loss=-2.818689 lr=9.78e-04 step=10100\n",
            "2026-02-19 10:37:46 [debug    ] train_step                     loss=-2.863608 lr=9.77e-04 step=10150\n",
            "2026-02-19 10:37:49 [debug    ] train_step                     loss=-3.152772 lr=9.77e-04 step=10200\n",
            "2026-02-19 10:37:52 [debug    ] train_step                     loss=-2.733808 lr=9.77e-04 step=10250\n",
            "2026-02-19 10:37:55 [debug    ] train_step                     loss=-2.883369 lr=9.77e-04 step=10300\n",
            "2026-02-19 10:37:58 [debug    ] train_step                     loss=-3.123912 lr=9.76e-04 step=10350\n",
            "2026-02-19 10:38:01 [debug    ] train_step                     loss=-3.146219 lr=9.76e-04 step=10400\n",
            "2026-02-19 10:38:04 [debug    ] train_step                     loss=-3.194973 lr=9.76e-04 step=10450\n",
            "2026-02-19 10:38:07 [debug    ] train_step                     loss=-3.196140 lr=9.76e-04 step=10500\n",
            "2026-02-19 10:38:10 [debug    ] train_step                     loss=-3.139855 lr=9.76e-04 step=10550\n",
            "2026-02-19 10:38:13 [debug    ] train_step                     loss=-3.076051 lr=9.75e-04 step=10600\n",
            "2026-02-19 10:38:16 [debug    ] train_step                     loss=-3.023181 lr=9.75e-04 step=10650\n",
            "2026-02-19 10:38:19 [debug    ] train_step                     loss=-3.171737 lr=9.75e-04 step=10700\n",
            "2026-02-19 10:38:22 [debug    ] train_step                     loss=-3.112445 lr=9.75e-04 step=10750\n",
            "2026-02-19 10:38:25 [debug    ] train_step                     loss=-3.203782 lr=9.74e-04 step=10800\n",
            "2026-02-19 10:38:28 [debug    ] train_step                     loss=-2.984063 lr=9.74e-04 step=10850\n",
            "2026-02-19 10:38:30 [debug    ] train_step                     loss=-2.764629 lr=9.74e-04 step=10900\n",
            "2026-02-19 10:38:33 [debug    ] train_step                     loss=-3.173253 lr=9.74e-04 step=10950\n",
            "2026-02-19 10:38:37 [debug    ] train_step                     loss=-3.046976 lr=9.73e-04 step=11000\n",
            "2026-02-19 10:38:39 [debug    ] train_step                     loss=-2.462304 lr=9.73e-04 step=11050\n",
            "2026-02-19 10:38:42 [info     ] epoch_complete                 epoch=11 lr=9.73e-04 train_loss=-3.012993 val_loss=-3.177186\n",
            "2026-02-19 10:38:46 [debug    ] train_step                     loss=-2.703138 lr=9.73e-04 step=11100\n",
            "2026-02-19 10:38:49 [debug    ] train_step                     loss=-3.025379 lr=9.73e-04 step=11150\n",
            "2026-02-19 10:38:52 [debug    ] train_step                     loss=-3.053968 lr=9.72e-04 step=11200\n",
            "2026-02-19 10:38:55 [debug    ] train_step                     loss=-3.162008 lr=9.72e-04 step=11250\n",
            "2026-02-19 10:38:58 [debug    ] train_step                     loss=-2.919569 lr=9.72e-04 step=11300\n",
            "2026-02-19 10:39:00 [debug    ] train_step                     loss=-3.060266 lr=9.72e-04 step=11350\n",
            "2026-02-19 10:39:03 [debug    ] train_step                     loss=-3.160250 lr=9.71e-04 step=11400\n",
            "2026-02-19 10:39:06 [debug    ] train_step                     loss=-3.184368 lr=9.71e-04 step=11450\n",
            "2026-02-19 10:39:09 [debug    ] train_step                     loss=-3.047567 lr=9.71e-04 step=11500\n",
            "2026-02-19 10:39:12 [debug    ] train_step                     loss=-3.061449 lr=9.70e-04 step=11550\n",
            "2026-02-19 10:39:15 [debug    ] train_step                     loss=-2.865440 lr=9.70e-04 step=11600\n",
            "2026-02-19 10:39:18 [debug    ] train_step                     loss=-2.433527 lr=9.70e-04 step=11650\n",
            "2026-02-19 10:39:21 [debug    ] train_step                     loss=-3.052633 lr=9.70e-04 step=11700\n",
            "2026-02-19 10:39:24 [debug    ] train_step                     loss=-3.016518 lr=9.69e-04 step=11750\n",
            "2026-02-19 10:39:27 [debug    ] train_step                     loss=-3.127207 lr=9.69e-04 step=11800\n",
            "2026-02-19 10:39:30 [debug    ] train_step                     loss=-2.823058 lr=9.69e-04 step=11850\n",
            "2026-02-19 10:39:33 [debug    ] train_step                     loss=-2.951067 lr=9.69e-04 step=11900\n",
            "2026-02-19 10:39:36 [debug    ] train_step                     loss=-3.109883 lr=9.68e-04 step=11950\n",
            "2026-02-19 10:39:39 [debug    ] train_step                     loss=-3.265282 lr=9.68e-04 step=12000\n",
            "2026-02-19 10:39:42 [debug    ] train_step                     loss=-3.045882 lr=9.68e-04 step=12050\n",
            "2026-02-19 10:39:44 [info     ] epoch_complete                 epoch=12 lr=9.68e-04 train_loss=-2.991342 val_loss=-3.143815\n",
            "2026-02-19 10:39:48 [debug    ] train_step                     loss=-2.830351 lr=9.67e-04 step=12100\n",
            "2026-02-19 10:39:51 [debug    ] train_step                     loss=-3.045249 lr=9.67e-04 step=12150\n",
            "2026-02-19 10:39:54 [debug    ] train_step                     loss=-2.920695 lr=9.67e-04 step=12200\n",
            "2026-02-19 10:39:57 [debug    ] train_step                     loss=-3.091790 lr=9.67e-04 step=12250\n",
            "2026-02-19 10:40:00 [debug    ] train_step                     loss=-3.008572 lr=9.66e-04 step=12300\n",
            "2026-02-19 10:40:03 [debug    ] train_step                     loss=-3.045711 lr=9.66e-04 step=12350\n",
            "2026-02-19 10:40:06 [debug    ] train_step                     loss=-2.100007 lr=9.66e-04 step=12400\n",
            "2026-02-19 10:40:09 [debug    ] train_step                     loss=-2.709428 lr=9.66e-04 step=12450\n",
            "2026-02-19 10:40:12 [debug    ] train_step                     loss=-3.120648 lr=9.65e-04 step=12500\n",
            "2026-02-19 10:40:14 [debug    ] train_step                     loss=-3.136130 lr=9.65e-04 step=12550\n",
            "2026-02-19 10:40:17 [debug    ] train_step                     loss=-3.069077 lr=9.65e-04 step=12600\n",
            "2026-02-19 10:40:20 [debug    ] train_step                     loss=-3.142179 lr=9.64e-04 step=12650\n",
            "2026-02-19 10:40:23 [debug    ] train_step                     loss=-2.808865 lr=9.64e-04 step=12700\n",
            "2026-02-19 10:40:26 [debug    ] train_step                     loss=-3.056202 lr=9.64e-04 step=12750\n",
            "2026-02-19 10:40:29 [debug    ] train_step                     loss=-2.830131 lr=9.64e-04 step=12800\n",
            "2026-02-19 10:40:32 [debug    ] train_step                     loss=-3.097179 lr=9.63e-04 step=12850\n",
            "2026-02-19 10:40:35 [debug    ] train_step                     loss=-3.106496 lr=9.63e-04 step=12900\n",
            "2026-02-19 10:40:38 [debug    ] train_step                     loss=-3.020196 lr=9.63e-04 step=12950\n",
            "2026-02-19 10:40:41 [debug    ] train_step                     loss=-2.903742 lr=9.62e-04 step=13000\n",
            "2026-02-19 10:40:44 [debug    ] train_step                     loss=-3.024008 lr=9.62e-04 step=13050\n",
            "2026-02-19 10:40:47 [info     ] epoch_complete                 epoch=13 lr=9.62e-04 train_loss=-3.004342 val_loss=-2.896532\n",
            "2026-02-19 10:40:50 [debug    ] train_step                     loss=-3.075595 lr=9.62e-04 step=13100\n",
            "2026-02-19 10:40:53 [debug    ] train_step                     loss=-3.002282 lr=9.61e-04 step=13150\n",
            "2026-02-19 10:40:56 [debug    ] train_step                     loss=-2.666520 lr=9.61e-04 step=13200\n",
            "2026-02-19 10:40:59 [debug    ] train_step                     loss=-2.942335 lr=9.61e-04 step=13250\n",
            "2026-02-19 10:41:02 [debug    ] train_step                     loss=-2.787421 lr=9.61e-04 step=13300\n",
            "2026-02-19 10:41:05 [debug    ] train_step                     loss=-2.847007 lr=9.60e-04 step=13350\n",
            "2026-02-19 10:41:08 [debug    ] train_step                     loss=-2.920056 lr=9.60e-04 step=13400\n",
            "2026-02-19 10:41:11 [debug    ] train_step                     loss=-3.103092 lr=9.60e-04 step=13450\n",
            "2026-02-19 10:41:14 [debug    ] train_step                     loss=-3.018362 lr=9.59e-04 step=13500\n",
            "2026-02-19 10:41:17 [debug    ] train_step                     loss=-2.754509 lr=9.59e-04 step=13550\n",
            "2026-02-19 10:41:20 [debug    ] train_step                     loss=-3.128300 lr=9.59e-04 step=13600\n",
            "2026-02-19 10:41:23 [debug    ] train_step                     loss=-2.998899 lr=9.58e-04 step=13650\n",
            "2026-02-19 10:41:26 [debug    ] train_step                     loss=-3.248740 lr=9.58e-04 step=13700\n",
            "2026-02-19 10:41:29 [debug    ] train_step                     loss=-3.204559 lr=9.58e-04 step=13750\n",
            "2026-02-19 10:41:31 [debug    ] train_step                     loss=-3.149508 lr=9.57e-04 step=13800\n",
            "2026-02-19 10:41:34 [debug    ] train_step                     loss=-3.105077 lr=9.57e-04 step=13850\n",
            "2026-02-19 10:41:37 [debug    ] train_step                     loss=-3.056812 lr=9.57e-04 step=13900\n",
            "2026-02-19 10:41:40 [debug    ] train_step                     loss=-3.278033 lr=9.56e-04 step=13950\n",
            "2026-02-19 10:41:43 [debug    ] train_step                     loss=-3.111339 lr=9.56e-04 step=14000\n",
            "2026-02-19 10:41:46 [debug    ] train_step                     loss=-2.949497 lr=9.56e-04 step=14050\n",
            "2026-02-19 10:41:50 [info     ] epoch_complete                 epoch=14 lr=9.56e-04 train_loss=-3.009564 val_loss=-3.174040\n",
            "2026-02-19 10:41:53 [debug    ] train_step                     loss=-3.291960 lr=9.56e-04 step=14100\n",
            "2026-02-19 10:41:55 [debug    ] train_step                     loss=-2.881482 lr=9.55e-04 step=14150\n",
            "2026-02-19 10:41:58 [debug    ] train_step                     loss=-3.074204 lr=9.55e-04 step=14200\n",
            "2026-02-19 10:42:01 [debug    ] train_step                     loss=-2.900932 lr=9.55e-04 step=14250\n",
            "2026-02-19 10:42:04 [debug    ] train_step                     loss=-3.152679 lr=9.54e-04 step=14300\n",
            "2026-02-19 10:42:07 [debug    ] train_step                     loss=-3.083719 lr=9.54e-04 step=14350\n",
            "2026-02-19 10:42:10 [debug    ] train_step                     loss=-2.842216 lr=9.54e-04 step=14400\n",
            "2026-02-19 10:42:13 [debug    ] train_step                     loss=-3.023721 lr=9.53e-04 step=14450\n",
            "2026-02-19 10:42:16 [debug    ] train_step                     loss=-3.005162 lr=9.53e-04 step=14500\n",
            "2026-02-19 10:42:19 [debug    ] train_step                     loss=-3.050201 lr=9.53e-04 step=14550\n",
            "2026-02-19 10:42:22 [debug    ] train_step                     loss=-2.535064 lr=9.52e-04 step=14600\n",
            "2026-02-19 10:42:25 [debug    ] train_step                     loss=-3.209340 lr=9.52e-04 step=14650\n",
            "2026-02-19 10:42:28 [debug    ] train_step                     loss=-3.175763 lr=9.52e-04 step=14700\n",
            "2026-02-19 10:42:31 [debug    ] train_step                     loss=-2.993644 lr=9.51e-04 step=14750\n",
            "2026-02-19 10:42:34 [debug    ] train_step                     loss=-3.047790 lr=9.51e-04 step=14800\n",
            "2026-02-19 10:42:37 [debug    ] train_step                     loss=-3.046126 lr=9.51e-04 step=14850\n",
            "2026-02-19 10:42:40 [debug    ] train_step                     loss=-2.829515 lr=9.50e-04 step=14900\n",
            "2026-02-19 10:42:43 [debug    ] train_step                     loss=-3.074785 lr=9.50e-04 step=14950\n",
            "2026-02-19 10:42:45 [debug    ] train_step                     loss=-2.982133 lr=9.50e-04 step=15000\n",
            "2026-02-19 10:42:48 [debug    ] train_step                     loss=-3.060384 lr=9.49e-04 step=15050\n",
            "2026-02-19 10:42:52 [info     ] epoch_complete                 epoch=15 lr=9.49e-04 train_loss=-3.013934 val_loss=-3.160447\n",
            "2026-02-19 10:42:55 [debug    ] train_step                     loss=-3.073946 lr=9.49e-04 step=15100\n",
            "2026-02-19 10:42:58 [debug    ] train_step                     loss=-3.178828 lr=9.48e-04 step=15150\n",
            "2026-02-19 10:43:01 [debug    ] train_step                     loss=-2.634788 lr=9.48e-04 step=15200\n",
            "2026-02-19 10:43:04 [debug    ] train_step                     loss=-2.961603 lr=9.48e-04 step=15250\n",
            "2026-02-19 10:43:07 [debug    ] train_step                     loss=-3.020905 lr=9.47e-04 step=15300\n",
            "2026-02-19 10:43:10 [debug    ] train_step                     loss=-3.261461 lr=9.47e-04 step=15350\n",
            "2026-02-19 10:43:13 [debug    ] train_step                     loss=-3.064710 lr=9.47e-04 step=15400\n",
            "2026-02-19 10:43:16 [debug    ] train_step                     loss=-2.930415 lr=9.46e-04 step=15450\n",
            "2026-02-19 10:43:19 [debug    ] train_step                     loss=-2.987008 lr=9.46e-04 step=15500\n",
            "2026-02-19 10:43:22 [debug    ] train_step                     loss=-3.141725 lr=9.46e-04 step=15550\n",
            "2026-02-19 10:43:25 [debug    ] train_step                     loss=-3.131531 lr=9.45e-04 step=15600\n",
            "2026-02-19 10:43:28 [debug    ] train_step                     loss=-3.163232 lr=9.45e-04 step=15650\n",
            "2026-02-19 10:43:31 [debug    ] train_step                     loss=-2.727542 lr=9.45e-04 step=15700\n",
            "2026-02-19 10:43:34 [debug    ] train_step                     loss=-3.170861 lr=9.44e-04 step=15750\n",
            "2026-02-19 10:43:37 [debug    ] train_step                     loss=-2.978896 lr=9.44e-04 step=15800\n",
            "2026-02-19 10:43:40 [debug    ] train_step                     loss=-3.163255 lr=9.44e-04 step=15850\n",
            "2026-02-19 10:43:43 [debug    ] train_step                     loss=-3.144020 lr=9.43e-04 step=15900\n",
            "2026-02-19 10:43:46 [debug    ] train_step                     loss=-3.078627 lr=9.43e-04 step=15950\n",
            "2026-02-19 10:43:48 [debug    ] train_step                     loss=-3.070991 lr=9.42e-04 step=16000\n",
            "2026-02-19 10:43:52 [debug    ] train_step                     loss=-3.010011 lr=9.42e-04 step=16050\n",
            "2026-02-19 10:43:55 [info     ] epoch_complete                 epoch=16 lr=9.42e-04 train_loss=-3.022964 val_loss=-3.193781\n",
            "2026-02-19 10:43:58 [debug    ] train_step                     loss=-3.128110 lr=9.42e-04 step=16100\n",
            "2026-02-19 10:44:01 [debug    ] train_step                     loss=-2.753922 lr=9.41e-04 step=16150\n",
            "2026-02-19 10:44:04 [debug    ] train_step                     loss=-2.954632 lr=9.41e-04 step=16200\n",
            "2026-02-19 10:44:07 [debug    ] train_step                     loss=-3.187839 lr=9.41e-04 step=16250\n",
            "2026-02-19 10:44:10 [debug    ] train_step                     loss=-3.151244 lr=9.40e-04 step=16300\n",
            "2026-02-19 10:44:13 [debug    ] train_step                     loss=-2.865943 lr=9.40e-04 step=16350\n",
            "2026-02-19 10:44:16 [debug    ] train_step                     loss=-3.125531 lr=9.40e-04 step=16400\n",
            "2026-02-19 10:44:19 [debug    ] train_step                     loss=-3.144449 lr=9.39e-04 step=16450\n",
            "2026-02-19 10:44:22 [debug    ] train_step                     loss=-3.076636 lr=9.39e-04 step=16500\n",
            "2026-02-19 10:44:25 [debug    ] train_step                     loss=-3.124928 lr=9.38e-04 step=16550\n",
            "2026-02-19 10:44:28 [debug    ] train_step                     loss=-3.204695 lr=9.38e-04 step=16600\n",
            "2026-02-19 10:44:31 [debug    ] train_step                     loss=-3.084712 lr=9.38e-04 step=16650\n",
            "2026-02-19 10:44:34 [debug    ] train_step                     loss=-2.901077 lr=9.37e-04 step=16700\n",
            "2026-02-19 10:44:37 [debug    ] train_step                     loss=-2.917323 lr=9.37e-04 step=16750\n",
            "2026-02-19 10:44:40 [debug    ] train_step                     loss=-3.128913 lr=9.37e-04 step=16800\n",
            "2026-02-19 10:44:43 [debug    ] train_step                     loss=-3.187724 lr=9.36e-04 step=16850\n",
            "2026-02-19 10:44:46 [debug    ] train_step                     loss=-3.051970 lr=9.36e-04 step=16900\n",
            "2026-02-19 10:44:49 [debug    ] train_step                     loss=-2.920779 lr=9.35e-04 step=16950\n",
            "2026-02-19 10:44:52 [debug    ] train_step                     loss=-3.112490 lr=9.35e-04 step=17000\n",
            "2026-02-19 10:44:55 [debug    ] train_step                     loss=-3.019861 lr=9.35e-04 step=17050\n",
            "2026-02-19 10:45:00 [info     ] epoch_complete                 epoch=17 lr=9.34e-04 train_loss=-3.024342 val_loss=-3.220490\n",
            "2026-02-19 10:45:01 [info     ] checkpoint_saved               epoch=16 path=best.pt score=-3.220489744412697\n",
            "2026-02-19 10:45:03 [debug    ] train_step                     loss=-3.148419 lr=9.34e-04 step=17100\n",
            "2026-02-19 10:45:06 [debug    ] train_step                     loss=-3.165707 lr=9.34e-04 step=17150\n",
            "2026-02-19 10:45:09 [debug    ] train_step                     loss=-3.080780 lr=9.33e-04 step=17200\n",
            "2026-02-19 10:45:12 [debug    ] train_step                     loss=-2.892754 lr=9.33e-04 step=17250\n",
            "2026-02-19 10:45:15 [debug    ] train_step                     loss=-2.964037 lr=9.33e-04 step=17300\n",
            "2026-02-19 10:45:18 [debug    ] train_step                     loss=-2.695303 lr=9.32e-04 step=17350\n",
            "2026-02-19 10:45:21 [debug    ] train_step                     loss=-2.780330 lr=9.32e-04 step=17400\n",
            "2026-02-19 10:45:24 [debug    ] train_step                     loss=-3.101679 lr=9.31e-04 step=17450\n",
            "2026-02-19 10:45:27 [debug    ] train_step                     loss=-3.173129 lr=9.31e-04 step=17500\n",
            "2026-02-19 10:45:30 [debug    ] train_step                     loss=-3.024844 lr=9.31e-04 step=17550\n",
            "2026-02-19 10:45:33 [debug    ] train_step                     loss=-3.111065 lr=9.30e-04 step=17600\n",
            "2026-02-19 10:45:36 [debug    ] train_step                     loss=-3.196203 lr=9.30e-04 step=17650\n",
            "2026-02-19 10:45:39 [debug    ] train_step                     loss=-2.792375 lr=9.29e-04 step=17700\n",
            "2026-02-19 10:45:42 [debug    ] train_step                     loss=-2.902361 lr=9.29e-04 step=17750\n",
            "2026-02-19 10:45:45 [debug    ] train_step                     loss=-2.985592 lr=9.29e-04 step=17800\n",
            "2026-02-19 10:45:48 [debug    ] train_step                     loss=-2.986818 lr=9.28e-04 step=17850\n",
            "2026-02-19 10:45:51 [debug    ] train_step                     loss=-2.674961 lr=9.28e-04 step=17900\n",
            "2026-02-19 10:45:54 [debug    ] train_step                     loss=-2.748911 lr=9.27e-04 step=17950\n",
            "2026-02-19 10:45:57 [debug    ] train_step                     loss=-2.634494 lr=9.27e-04 step=18000\n",
            "2026-02-19 10:46:00 [debug    ] train_step                     loss=-3.115016 lr=9.27e-04 step=18050\n",
            "2026-02-19 10:46:04 [info     ] epoch_complete                 epoch=18 lr=9.26e-04 train_loss=-3.028029 val_loss=-3.174185\n",
            "2026-02-19 10:46:06 [debug    ] train_step                     loss=-3.109900 lr=9.26e-04 step=18100\n",
            "2026-02-19 10:46:09 [debug    ] train_step                     loss=-3.116350 lr=9.26e-04 step=18150\n",
            "2026-02-19 10:46:12 [debug    ] train_step                     loss=-3.199158 lr=9.25e-04 step=18200\n",
            "2026-02-19 10:46:15 [debug    ] train_step                     loss=-2.977492 lr=9.25e-04 step=18250\n",
            "2026-02-19 10:46:18 [debug    ] train_step                     loss=-3.136688 lr=9.25e-04 step=18300\n",
            "2026-02-19 10:46:21 [debug    ] train_step                     loss=-3.117027 lr=9.24e-04 step=18350\n",
            "2026-02-19 10:46:24 [debug    ] train_step                     loss=-2.928535 lr=9.24e-04 step=18400\n",
            "2026-02-19 10:46:27 [debug    ] train_step                     loss=-3.139599 lr=9.23e-04 step=18450\n",
            "2026-02-19 10:46:30 [debug    ] train_step                     loss=-3.080261 lr=9.23e-04 step=18500\n",
            "2026-02-19 10:46:33 [debug    ] train_step                     loss=-3.141012 lr=9.23e-04 step=18550\n",
            "2026-02-19 10:46:36 [debug    ] train_step                     loss=-3.127312 lr=9.22e-04 step=18600\n",
            "2026-02-19 10:46:39 [debug    ] train_step                     loss=-3.230723 lr=9.22e-04 step=18650\n",
            "2026-02-19 10:46:42 [debug    ] train_step                     loss=-3.061176 lr=9.21e-04 step=18700\n",
            "2026-02-19 10:46:45 [debug    ] train_step                     loss=-2.994081 lr=9.21e-04 step=18750\n",
            "2026-02-19 10:46:48 [debug    ] train_step                     loss=-2.923999 lr=9.20e-04 step=18800\n",
            "2026-02-19 10:46:51 [debug    ] train_step                     loss=-3.184983 lr=9.20e-04 step=18850\n",
            "2026-02-19 10:46:54 [debug    ] train_step                     loss=-2.448463 lr=9.20e-04 step=18900\n",
            "2026-02-19 10:46:57 [debug    ] train_step                     loss=-2.872011 lr=9.19e-04 step=18950\n",
            "2026-02-19 10:47:00 [debug    ] train_step                     loss=-1.550283 lr=9.19e-04 step=19000\n",
            "2026-02-19 10:47:03 [debug    ] train_step                     loss=-3.111699 lr=9.18e-04 step=19050\n",
            "2026-02-19 10:47:08 [info     ] epoch_complete                 epoch=19 lr=9.18e-04 train_loss=-3.016718 val_loss=-3.179529\n",
            "2026-02-19 10:47:10 [debug    ] train_step                     loss=-3.160875 lr=9.18e-04 step=19100\n",
            "2026-02-19 10:47:13 [debug    ] train_step                     loss=-2.799771 lr=9.17e-04 step=19150\n",
            "2026-02-19 10:47:16 [debug    ] train_step                     loss=-3.144443 lr=9.17e-04 step=19200\n",
            "2026-02-19 10:47:19 [debug    ] train_step                     loss=-2.995571 lr=9.17e-04 step=19250\n",
            "2026-02-19 10:47:22 [debug    ] train_step                     loss=-3.139823 lr=9.16e-04 step=19300\n",
            "2026-02-19 10:47:25 [debug    ] train_step                     loss=-3.131856 lr=9.16e-04 step=19350\n",
            "2026-02-19 10:47:28 [debug    ] train_step                     loss=-3.273950 lr=9.15e-04 step=19400\n",
            "2026-02-19 10:47:31 [debug    ] train_step                     loss=-2.983358 lr=9.15e-04 step=19450\n",
            "2026-02-19 10:47:34 [debug    ] train_step                     loss=-2.780781 lr=9.14e-04 step=19500\n",
            "2026-02-19 10:47:37 [debug    ] train_step                     loss=-3.123389 lr=9.14e-04 step=19550\n",
            "2026-02-19 10:47:40 [debug    ] train_step                     loss=-3.279309 lr=9.14e-04 step=19600\n",
            "2026-02-19 10:47:43 [debug    ] train_step                     loss=-3.142109 lr=9.13e-04 step=19650\n",
            "2026-02-19 10:47:46 [debug    ] train_step                     loss=-3.063446 lr=9.13e-04 step=19700\n",
            "2026-02-19 10:47:49 [debug    ] train_step                     loss=-3.182806 lr=9.12e-04 step=19750\n",
            "2026-02-19 10:47:52 [debug    ] train_step                     loss=-3.004529 lr=9.12e-04 step=19800\n",
            "2026-02-19 10:47:55 [debug    ] train_step                     loss=-2.919816 lr=9.11e-04 step=19850\n",
            "2026-02-19 10:47:58 [debug    ] train_step                     loss=-3.053792 lr=9.11e-04 step=19900\n",
            "2026-02-19 10:48:01 [debug    ] train_step                     loss=-2.979639 lr=9.10e-04 step=19950\n",
            "2026-02-19 10:48:04 [debug    ] train_step                     loss=-3.060145 lr=9.10e-04 step=20000\n",
            "2026-02-19 10:48:07 [debug    ] train_step                     loss=-2.934765 lr=9.10e-04 step=20050\n",
            "2026-02-19 10:48:10 [debug    ] train_step                     loss=-3.056935 lr=9.09e-04 step=20100\n",
            "2026-02-19 10:48:12 [info     ] epoch_complete                 epoch=20 lr=9.09e-04 train_loss=-3.012066 val_loss=-3.168418\n",
            "2026-02-19 10:48:17 [debug    ] train_step                     loss=-3.138459 lr=9.09e-04 step=20150\n",
            "2026-02-19 10:48:20 [debug    ] train_step                     loss=-3.158302 lr=9.08e-04 step=20200\n",
            "2026-02-19 10:48:23 [debug    ] train_step                     loss=-3.009552 lr=9.08e-04 step=20250\n",
            "2026-02-19 10:48:26 [debug    ] train_step                     loss=-3.065746 lr=9.07e-04 step=20300\n",
            "2026-02-19 10:48:28 [debug    ] train_step                     loss=-3.214819 lr=9.07e-04 step=20350\n",
            "2026-02-19 10:48:31 [debug    ] train_step                     loss=-3.054956 lr=9.06e-04 step=20400\n",
            "2026-02-19 10:48:34 [debug    ] train_step                     loss=-3.121331 lr=9.06e-04 step=20450\n",
            "2026-02-19 10:48:37 [debug    ] train_step                     loss=-3.177549 lr=9.05e-04 step=20500\n",
            "2026-02-19 10:48:40 [debug    ] train_step                     loss=-3.134350 lr=9.05e-04 step=20550\n",
            "2026-02-19 10:48:43 [debug    ] train_step                     loss=-3.033016 lr=9.05e-04 step=20600\n",
            "2026-02-19 10:48:46 [debug    ] train_step                     loss=-2.976024 lr=9.04e-04 step=20650\n",
            "2026-02-19 10:48:49 [debug    ] train_step                     loss=-3.036068 lr=9.04e-04 step=20700\n",
            "2026-02-19 10:48:52 [debug    ] train_step                     loss=-3.190238 lr=9.03e-04 step=20750\n",
            "2026-02-19 10:48:55 [debug    ] train_step                     loss=-2.989605 lr=9.03e-04 step=20800\n",
            "2026-02-19 10:48:58 [debug    ] train_step                     loss=-3.235460 lr=9.02e-04 step=20850\n",
            "2026-02-19 10:49:01 [debug    ] train_step                     loss=-2.841537 lr=9.02e-04 step=20900\n",
            "2026-02-19 10:49:04 [debug    ] train_step                     loss=-3.021616 lr=9.01e-04 step=20950\n",
            "2026-02-19 10:49:07 [debug    ] train_step                     loss=-2.990293 lr=9.01e-04 step=21000\n",
            "2026-02-19 10:49:10 [debug    ] train_step                     loss=-2.910591 lr=9.00e-04 step=21050\n",
            "2026-02-19 10:49:13 [debug    ] train_step                     loss=-2.924569 lr=9.00e-04 step=21100\n",
            "2026-02-19 10:49:16 [info     ] epoch_complete                 epoch=21 lr=9.00e-04 train_loss=-3.027612 val_loss=-3.211039\n",
            "2026-02-19 10:49:20 [debug    ] train_step                     loss=-2.672817 lr=8.99e-04 step=21150\n",
            "2026-02-19 10:49:23 [debug    ] train_step                     loss=-2.743192 lr=8.99e-04 step=21200\n",
            "2026-02-19 10:49:26 [debug    ] train_step                     loss=-3.009726 lr=8.98e-04 step=21250\n",
            "2026-02-19 10:49:29 [debug    ] train_step                     loss=-2.849209 lr=8.98e-04 step=21300\n",
            "2026-02-19 10:49:32 [debug    ] train_step                     loss=-3.081292 lr=8.98e-04 step=21350\n",
            "2026-02-19 10:49:35 [debug    ] train_step                     loss=-2.992743 lr=8.97e-04 step=21400\n",
            "2026-02-19 10:49:38 [debug    ] train_step                     loss=-2.886397 lr=8.97e-04 step=21450\n",
            "2026-02-19 10:49:41 [debug    ] train_step                     loss=-3.012646 lr=8.96e-04 step=21500\n",
            "2026-02-19 10:49:44 [debug    ] train_step                     loss=-2.745633 lr=8.96e-04 step=21550\n",
            "2026-02-19 10:49:47 [debug    ] train_step                     loss=-2.949471 lr=8.95e-04 step=21600\n",
            "2026-02-19 10:49:50 [debug    ] train_step                     loss=-3.200593 lr=8.95e-04 step=21650\n",
            "2026-02-19 10:49:53 [debug    ] train_step                     loss=-3.144419 lr=8.94e-04 step=21700\n",
            "2026-02-19 10:49:56 [debug    ] train_step                     loss=-3.097356 lr=8.94e-04 step=21750\n",
            "2026-02-19 10:49:59 [debug    ] train_step                     loss=-2.678128 lr=8.93e-04 step=21800\n",
            "2026-02-19 10:50:02 [debug    ] train_step                     loss=-3.109288 lr=8.93e-04 step=21850\n",
            "2026-02-19 10:50:05 [debug    ] train_step                     loss=-3.091057 lr=8.92e-04 step=21900\n",
            "2026-02-19 10:50:08 [debug    ] train_step                     loss=-2.815193 lr=8.92e-04 step=21950\n",
            "2026-02-19 10:50:11 [debug    ] train_step                     loss=-2.949415 lr=8.91e-04 step=22000\n",
            "2026-02-19 10:50:14 [debug    ] train_step                     loss=-2.630276 lr=8.91e-04 step=22050\n",
            "2026-02-19 10:50:17 [debug    ] train_step                     loss=-3.014751 lr=8.90e-04 step=22100\n",
            "2026-02-19 10:50:20 [info     ] epoch_complete                 epoch=22 lr=8.90e-04 train_loss=-3.022175 val_loss=-3.156145\n",
            "2026-02-19 10:50:24 [debug    ] train_step                     loss=-2.858723 lr=8.90e-04 step=22150\n",
            "2026-02-19 10:50:27 [debug    ] train_step                     loss=-3.212648 lr=8.89e-04 step=22200\n",
            "2026-02-19 10:50:30 [debug    ] train_step                     loss=-3.179036 lr=8.89e-04 step=22250\n",
            "2026-02-19 10:50:33 [debug    ] train_step                     loss=-3.121647 lr=8.88e-04 step=22300\n",
            "2026-02-19 10:50:36 [debug    ] train_step                     loss=-2.832065 lr=8.88e-04 step=22350\n",
            "2026-02-19 10:50:39 [debug    ] train_step                     loss=-3.300493 lr=8.87e-04 step=22400\n",
            "2026-02-19 10:50:42 [debug    ] train_step                     loss=-3.149948 lr=8.87e-04 step=22450\n",
            "2026-02-19 10:50:45 [debug    ] train_step                     loss=-3.034327 lr=8.86e-04 step=22500\n",
            "2026-02-19 10:50:48 [debug    ] train_step                     loss=-2.888272 lr=8.86e-04 step=22550\n",
            "2026-02-19 10:50:51 [debug    ] train_step                     loss=-2.984802 lr=8.85e-04 step=22600\n",
            "2026-02-19 10:50:54 [debug    ] train_step                     loss=-2.812077 lr=8.85e-04 step=22650\n",
            "2026-02-19 10:50:57 [debug    ] train_step                     loss=-3.194512 lr=8.84e-04 step=22700\n",
            "2026-02-19 10:51:00 [debug    ] train_step                     loss=-2.948256 lr=8.84e-04 step=22750\n",
            "2026-02-19 10:51:03 [debug    ] train_step                     loss=-3.076169 lr=8.83e-04 step=22800\n",
            "2026-02-19 10:51:06 [debug    ] train_step                     loss=-2.586372 lr=8.83e-04 step=22850\n",
            "2026-02-19 10:51:09 [debug    ] train_step                     loss=-3.209934 lr=8.82e-04 step=22900\n",
            "2026-02-19 10:51:12 [debug    ] train_step                     loss=-3.008980 lr=8.82e-04 step=22950\n",
            "2026-02-19 10:51:15 [debug    ] train_step                     loss=-3.237629 lr=8.81e-04 step=23000\n",
            "2026-02-19 10:51:17 [debug    ] train_step                     loss=-3.233346 lr=8.81e-04 step=23050\n",
            "2026-02-19 10:51:20 [debug    ] train_step                     loss=-3.193182 lr=8.80e-04 step=23100\n",
            "2026-02-19 10:51:24 [info     ] epoch_complete                 epoch=23 lr=8.80e-04 train_loss=-3.026810 val_loss=-3.234603\n",
            "2026-02-19 10:51:25 [info     ] checkpoint_saved               epoch=22 path=best.pt score=-3.2346034241934953\n",
            "2026-02-19 10:51:28 [debug    ] train_step                     loss=-3.173953 lr=8.80e-04 step=23150\n",
            "2026-02-19 10:51:31 [debug    ] train_step                     loss=-3.181237 lr=8.79e-04 step=23200\n",
            "2026-02-19 10:51:34 [debug    ] train_step                     loss=-2.876724 lr=8.79e-04 step=23250\n",
            "2026-02-19 10:51:37 [debug    ] train_step                     loss=-3.111350 lr=8.78e-04 step=23300\n",
            "2026-02-19 10:51:40 [debug    ] train_step                     loss=-3.065127 lr=8.78e-04 step=23350\n",
            "2026-02-19 10:51:43 [debug    ] train_step                     loss=-2.794232 lr=8.77e-04 step=23400\n",
            "2026-02-19 10:51:46 [debug    ] train_step                     loss=-3.009082 lr=8.77e-04 step=23450\n",
            "2026-02-19 10:51:49 [debug    ] train_step                     loss=-3.139528 lr=8.76e-04 step=23500\n",
            "2026-02-19 10:51:52 [debug    ] train_step                     loss=-2.866893 lr=8.76e-04 step=23550\n",
            "2026-02-19 10:51:55 [debug    ] train_step                     loss=-3.033406 lr=8.75e-04 step=23600\n",
            "2026-02-19 10:51:57 [debug    ] train_step                     loss=-3.160601 lr=8.75e-04 step=23650\n",
            "2026-02-19 10:52:00 [debug    ] train_step                     loss=-3.131769 lr=8.74e-04 step=23700\n",
            "2026-02-19 10:52:03 [debug    ] train_step                     loss=-3.084954 lr=8.74e-04 step=23750\n",
            "2026-02-19 10:52:06 [debug    ] train_step                     loss=-3.190444 lr=8.73e-04 step=23800\n",
            "2026-02-19 10:52:09 [debug    ] train_step                     loss=-2.945400 lr=8.73e-04 step=23850\n",
            "2026-02-19 10:52:12 [debug    ] train_step                     loss=-3.031556 lr=8.72e-04 step=23900\n",
            "2026-02-19 10:52:15 [debug    ] train_step                     loss=-3.002141 lr=8.72e-04 step=23950\n",
            "2026-02-19 10:52:18 [debug    ] train_step                     loss=-3.299130 lr=8.71e-04 step=24000\n",
            "2026-02-19 10:52:21 [debug    ] train_step                     loss=-3.137685 lr=8.71e-04 step=24050\n",
            "2026-02-19 10:52:24 [debug    ] train_step                     loss=-3.070474 lr=8.70e-04 step=24100\n",
            "2026-02-19 10:52:28 [info     ] epoch_complete                 epoch=24 lr=8.70e-04 train_loss=-3.029078 val_loss=-3.079376\n",
            "2026-02-19 10:52:30 [debug    ] train_step                     loss=-3.106598 lr=8.70e-04 step=24150\n",
            "2026-02-19 10:52:33 [debug    ] train_step                     loss=-3.104031 lr=8.69e-04 step=24200\n",
            "2026-02-19 10:52:36 [debug    ] train_step                     loss=-3.182377 lr=8.68e-04 step=24250\n",
            "2026-02-19 10:52:39 [debug    ] train_step                     loss=-3.081906 lr=8.68e-04 step=24300\n",
            "2026-02-19 10:52:42 [debug    ] train_step                     loss=-2.972407 lr=8.67e-04 step=24350\n",
            "2026-02-19 10:52:45 [debug    ] train_step                     loss=-3.127777 lr=8.67e-04 step=24400\n",
            "2026-02-19 10:52:49 [debug    ] train_step                     loss=-3.028722 lr=8.66e-04 step=24450\n",
            "2026-02-19 10:52:52 [debug    ] train_step                     loss=-2.223548 lr=8.66e-04 step=24500\n",
            "2026-02-19 10:52:55 [debug    ] train_step                     loss=-3.193593 lr=8.65e-04 step=24550\n",
            "2026-02-19 10:52:58 [debug    ] train_step                     loss=-3.185719 lr=8.65e-04 step=24600\n",
            "2026-02-19 10:53:01 [debug    ] train_step                     loss=-2.893387 lr=8.64e-04 step=24650\n",
            "2026-02-19 10:53:04 [debug    ] train_step                     loss=-2.361722 lr=8.64e-04 step=24700\n",
            "2026-02-19 10:53:06 [debug    ] train_step                     loss=-3.201381 lr=8.63e-04 step=24750\n",
            "2026-02-19 10:53:09 [debug    ] train_step                     loss=-3.216616 lr=8.63e-04 step=24800\n",
            "2026-02-19 10:53:12 [debug    ] train_step                     loss=-3.303902 lr=8.62e-04 step=24850\n",
            "2026-02-19 10:53:15 [debug    ] train_step                     loss=-3.131544 lr=8.62e-04 step=24900\n",
            "2026-02-19 10:53:18 [debug    ] train_step                     loss=-3.227833 lr=8.61e-04 step=24950\n",
            "2026-02-19 10:53:21 [debug    ] train_step                     loss=-2.887117 lr=8.60e-04 step=25000\n",
            "2026-02-19 10:53:24 [debug    ] train_step                     loss=-2.996840 lr=8.60e-04 step=25050\n",
            "2026-02-19 10:53:27 [debug    ] train_step                     loss=-3.166470 lr=8.59e-04 step=25100\n",
            "2026-02-19 10:53:31 [info     ] epoch_complete                 epoch=25 lr=8.59e-04 train_loss=-3.029213 val_loss=-3.170705\n",
            "2026-02-19 10:53:34 [debug    ] train_step                     loss=-3.021405 lr=8.59e-04 step=25150\n",
            "2026-02-19 10:53:37 [debug    ] train_step                     loss=-3.187058 lr=8.58e-04 step=25200\n",
            "2026-02-19 10:53:40 [debug    ] train_step                     loss=-3.002700 lr=8.58e-04 step=25250\n",
            "2026-02-19 10:53:43 [debug    ] train_step                     loss=-3.167368 lr=8.57e-04 step=25300\n",
            "2026-02-19 10:53:46 [debug    ] train_step                     loss=-2.926514 lr=8.57e-04 step=25350\n",
            "2026-02-19 10:53:49 [debug    ] train_step                     loss=-3.142439 lr=8.56e-04 step=25400\n",
            "2026-02-19 10:53:52 [debug    ] train_step                     loss=-3.014937 lr=8.56e-04 step=25450\n",
            "2026-02-19 10:53:55 [debug    ] train_step                     loss=-3.090786 lr=8.55e-04 step=25500\n",
            "2026-02-19 10:53:58 [debug    ] train_step                     loss=-3.112435 lr=8.54e-04 step=25550\n",
            "2026-02-19 10:54:01 [debug    ] train_step                     loss=-3.114702 lr=8.54e-04 step=25600\n",
            "2026-02-19 10:54:04 [debug    ] train_step                     loss=-2.832819 lr=8.53e-04 step=25650\n",
            "2026-02-19 10:54:07 [debug    ] train_step                     loss=-3.121168 lr=8.53e-04 step=25700\n",
            "2026-02-19 10:54:10 [debug    ] train_step                     loss=-2.994871 lr=8.52e-04 step=25750\n",
            "2026-02-19 10:54:13 [debug    ] train_step                     loss=-2.953139 lr=8.52e-04 step=25800\n",
            "2026-02-19 10:54:16 [debug    ] train_step                     loss=-3.231384 lr=8.51e-04 step=25850\n",
            "2026-02-19 10:54:19 [debug    ] train_step                     loss=-3.019670 lr=8.51e-04 step=25900\n",
            "2026-02-19 10:54:22 [debug    ] train_step                     loss=-3.068254 lr=8.50e-04 step=25950\n",
            "2026-02-19 10:54:25 [debug    ] train_step                     loss=-3.121370 lr=8.49e-04 step=26000\n",
            "2026-02-19 10:54:28 [debug    ] train_step                     loss=-3.218394 lr=8.49e-04 step=26050\n",
            "2026-02-19 10:54:31 [debug    ] train_step                     loss=-3.048656 lr=8.48e-04 step=26100\n",
            "2026-02-19 10:54:35 [info     ] epoch_complete                 epoch=26 lr=8.48e-04 train_loss=-3.038692 val_loss=-3.212448\n",
            "2026-02-19 10:54:37 [debug    ] train_step                     loss=-2.908568 lr=8.48e-04 step=26150\n",
            "2026-02-19 10:54:40 [debug    ] train_step                     loss=-3.110284 lr=8.47e-04 step=26200\n",
            "2026-02-19 10:54:43 [debug    ] train_step                     loss=-3.156281 lr=8.47e-04 step=26250\n",
            "2026-02-19 10:54:46 [debug    ] train_step                     loss=-2.919881 lr=8.46e-04 step=26300\n",
            "2026-02-19 10:54:49 [debug    ] train_step                     loss=-2.777865 lr=8.46e-04 step=26350\n",
            "2026-02-19 10:54:52 [debug    ] train_step                     loss=-3.128081 lr=8.45e-04 step=26400\n",
            "2026-02-19 10:54:55 [debug    ] train_step                     loss=-3.231151 lr=8.44e-04 step=26450\n",
            "2026-02-19 10:54:58 [debug    ] train_step                     loss=-3.173683 lr=8.44e-04 step=26500\n",
            "2026-02-19 10:55:01 [debug    ] train_step                     loss=-2.930051 lr=8.43e-04 step=26550\n",
            "2026-02-19 10:55:04 [debug    ] train_step                     loss=-3.117469 lr=8.43e-04 step=26600\n",
            "2026-02-19 10:55:07 [debug    ] train_step                     loss=-3.170848 lr=8.42e-04 step=26650\n",
            "2026-02-19 10:55:10 [debug    ] train_step                     loss=-2.986188 lr=8.42e-04 step=26700\n",
            "2026-02-19 10:55:13 [debug    ] train_step                     loss=-2.945079 lr=8.41e-04 step=26750\n",
            "2026-02-19 10:55:16 [debug    ] train_step                     loss=-3.139066 lr=8.40e-04 step=26800\n",
            "2026-02-19 10:55:19 [debug    ] train_step                     loss=-3.055964 lr=8.40e-04 step=26850\n",
            "2026-02-19 10:55:22 [debug    ] train_step                     loss=-3.130996 lr=8.39e-04 step=26900\n",
            "2026-02-19 10:55:25 [debug    ] train_step                     loss=-2.723003 lr=8.39e-04 step=26950\n",
            "2026-02-19 10:55:28 [debug    ] train_step                     loss=-3.203023 lr=8.38e-04 step=27000\n",
            "2026-02-19 10:55:31 [debug    ] train_step                     loss=-3.298344 lr=8.38e-04 step=27050\n",
            "2026-02-19 10:55:34 [debug    ] train_step                     loss=-3.193997 lr=8.37e-04 step=27100\n",
            "2026-02-19 10:55:39 [info     ] epoch_complete                 epoch=27 lr=8.37e-04 train_loss=-3.021870 val_loss=-3.147644\n",
            "2026-02-19 10:55:41 [debug    ] train_step                     loss=-2.962875 lr=8.36e-04 step=27150\n",
            "2026-02-19 10:55:44 [debug    ] train_step                     loss=-2.998722 lr=8.36e-04 step=27200\n",
            "2026-02-19 10:55:47 [debug    ] train_step                     loss=-2.966379 lr=8.35e-04 step=27250\n",
            "2026-02-19 10:55:50 [debug    ] train_step                     loss=-3.155365 lr=8.35e-04 step=27300\n",
            "2026-02-19 10:55:53 [debug    ] train_step                     loss=-3.001573 lr=8.34e-04 step=27350\n",
            "2026-02-19 10:55:56 [debug    ] train_step                     loss=-3.204736 lr=8.34e-04 step=27400\n",
            "2026-02-19 10:55:59 [debug    ] train_step                     loss=-3.122914 lr=8.33e-04 step=27450\n",
            "2026-02-19 10:56:02 [debug    ] train_step                     loss=-3.036942 lr=8.32e-04 step=27500\n",
            "2026-02-19 10:56:05 [debug    ] train_step                     loss=-3.144480 lr=8.32e-04 step=27550\n",
            "2026-02-19 10:56:08 [debug    ] train_step                     loss=-2.804042 lr=8.31e-04 step=27600\n",
            "2026-02-19 10:56:11 [debug    ] train_step                     loss=-3.111153 lr=8.31e-04 step=27650\n",
            "2026-02-19 10:56:14 [debug    ] train_step                     loss=-2.615850 lr=8.30e-04 step=27700\n",
            "2026-02-19 10:56:17 [debug    ] train_step                     loss=-3.064851 lr=8.29e-04 step=27750\n",
            "2026-02-19 10:56:20 [debug    ] train_step                     loss=-3.064289 lr=8.29e-04 step=27800\n",
            "2026-02-19 10:56:23 [debug    ] train_step                     loss=-3.083126 lr=8.28e-04 step=27850\n",
            "2026-02-19 10:56:26 [debug    ] train_step                     loss=-3.128538 lr=8.28e-04 step=27900\n",
            "2026-02-19 10:56:29 [debug    ] train_step                     loss=-3.006289 lr=8.27e-04 step=27950\n",
            "2026-02-19 10:56:32 [debug    ] train_step                     loss=-2.964815 lr=8.26e-04 step=28000\n",
            "2026-02-19 10:56:35 [debug    ] train_step                     loss=-3.171397 lr=8.26e-04 step=28050\n",
            "2026-02-19 10:56:38 [debug    ] train_step                     loss=-2.819003 lr=8.25e-04 step=28100\n",
            "2026-02-19 10:56:43 [info     ] epoch_complete                 epoch=28 lr=8.25e-04 train_loss=-3.031641 val_loss=-3.034372\n",
            "2026-02-19 10:56:44 [debug    ] train_step                     loss=-3.030796 lr=8.25e-04 step=28150\n",
            "2026-02-19 10:56:47 [debug    ] train_step                     loss=-3.096111 lr=8.24e-04 step=28200\n",
            "2026-02-19 10:56:50 [debug    ] train_step                     loss=-3.088954 lr=8.24e-04 step=28250\n",
            "2026-02-19 10:56:53 [debug    ] train_step                     loss=-2.980900 lr=8.23e-04 step=28300\n",
            "2026-02-19 10:56:56 [debug    ] train_step                     loss=-3.099020 lr=8.22e-04 step=28350\n",
            "2026-02-19 10:56:59 [debug    ] train_step                     loss=-3.004424 lr=8.22e-04 step=28400\n",
            "2026-02-19 10:57:02 [debug    ] train_step                     loss=-3.138083 lr=8.21e-04 step=28450\n",
            "2026-02-19 10:57:05 [debug    ] train_step                     loss=-3.060745 lr=8.21e-04 step=28500\n",
            "2026-02-19 10:57:08 [debug    ] train_step                     loss=-3.091627 lr=8.20e-04 step=28550\n",
            "2026-02-19 10:57:11 [debug    ] train_step                     loss=-3.105449 lr=8.19e-04 step=28600\n",
            "2026-02-19 10:57:14 [debug    ] train_step                     loss=-3.205450 lr=8.19e-04 step=28650\n",
            "2026-02-19 10:57:17 [debug    ] train_step                     loss=-3.140319 lr=8.18e-04 step=28700\n",
            "2026-02-19 10:57:20 [debug    ] train_step                     loss=-2.996385 lr=8.18e-04 step=28750\n",
            "2026-02-19 10:57:23 [debug    ] train_step                     loss=-2.983315 lr=8.17e-04 step=28800\n",
            "2026-02-19 10:57:26 [debug    ] train_step                     loss=-3.111369 lr=8.16e-04 step=28850\n",
            "2026-02-19 10:57:29 [debug    ] train_step                     loss=-3.111851 lr=8.16e-04 step=28900\n",
            "2026-02-19 10:57:32 [debug    ] train_step                     loss=-3.100421 lr=8.15e-04 step=28950\n",
            "2026-02-19 10:57:35 [debug    ] train_step                     loss=-3.055581 lr=8.14e-04 step=29000\n",
            "2026-02-19 10:57:38 [debug    ] train_step                     loss=-3.075735 lr=8.14e-04 step=29050\n",
            "2026-02-19 10:57:41 [debug    ] train_step                     loss=-3.093140 lr=8.13e-04 step=29100\n",
            "2026-02-19 10:57:46 [info     ] epoch_complete                 epoch=29 lr=8.13e-04 train_loss=-3.035710 val_loss=-3.143553\n",
            "2026-02-19 10:57:47 [debug    ] train_step                     loss=-3.076343 lr=8.13e-04 step=29150\n",
            "2026-02-19 10:57:50 [debug    ] train_step                     loss=-2.933247 lr=8.12e-04 step=29200\n",
            "2026-02-19 10:57:53 [debug    ] train_step                     loss=-3.068057 lr=8.11e-04 step=29250\n",
            "2026-02-19 10:57:56 [debug    ] train_step                     loss=-3.213869 lr=8.11e-04 step=29300\n",
            "2026-02-19 10:57:59 [debug    ] train_step                     loss=-2.366384 lr=8.10e-04 step=29350\n",
            "2026-02-19 10:58:02 [debug    ] train_step                     loss=-3.131217 lr=8.10e-04 step=29400\n",
            "2026-02-19 10:58:05 [debug    ] train_step                     loss=-2.964833 lr=8.09e-04 step=29450\n",
            "2026-02-19 10:58:08 [debug    ] train_step                     loss=-3.176132 lr=8.08e-04 step=29500\n",
            "2026-02-19 10:58:11 [debug    ] train_step                     loss=-3.098326 lr=8.08e-04 step=29550\n",
            "2026-02-19 10:58:14 [debug    ] train_step                     loss=-3.166890 lr=8.07e-04 step=29600\n",
            "2026-02-19 10:58:17 [debug    ] train_step                     loss=-2.970950 lr=8.07e-04 step=29650\n",
            "2026-02-19 10:58:20 [debug    ] train_step                     loss=-3.090688 lr=8.06e-04 step=29700\n",
            "2026-02-19 10:58:23 [debug    ] train_step                     loss=-3.156998 lr=8.05e-04 step=29750\n",
            "2026-02-19 10:58:26 [debug    ] train_step                     loss=-3.088046 lr=8.05e-04 step=29800\n",
            "2026-02-19 10:58:29 [debug    ] train_step                     loss=-3.209709 lr=8.04e-04 step=29850\n",
            "2026-02-19 10:58:32 [debug    ] train_step                     loss=-3.212867 lr=8.03e-04 step=29900\n",
            "2026-02-19 10:58:35 [debug    ] train_step                     loss=-2.999266 lr=8.03e-04 step=29950\n",
            "2026-02-19 10:58:38 [debug    ] train_step                     loss=-3.053718 lr=8.02e-04 step=30000\n",
            "2026-02-19 10:58:41 [debug    ] train_step                     loss=-3.244545 lr=8.02e-04 step=30050\n",
            "2026-02-19 10:58:44 [debug    ] train_step                     loss=-3.094226 lr=8.01e-04 step=30100\n",
            "2026-02-19 10:58:47 [debug    ] train_step                     loss=-3.079439 lr=8.00e-04 step=30150\n",
            "2026-02-19 10:58:49 [info     ] epoch_complete                 epoch=30 lr=8.00e-04 train_loss=-3.030678 val_loss=-3.208795\n",
            "2026-02-19 10:58:54 [debug    ] train_step                     loss=-3.079971 lr=8.00e-04 step=30200\n",
            "2026-02-19 10:58:57 [debug    ] train_step                     loss=-3.114997 lr=7.99e-04 step=30250\n",
            "2026-02-19 10:59:00 [debug    ] train_step                     loss=-2.855060 lr=7.98e-04 step=30300\n",
            "2026-02-19 10:59:03 [debug    ] train_step                     loss=-2.988062 lr=7.98e-04 step=30350\n",
            "2026-02-19 10:59:06 [debug    ] train_step                     loss=-3.179692 lr=7.97e-04 step=30400\n",
            "2026-02-19 10:59:09 [debug    ] train_step                     loss=-3.221446 lr=7.97e-04 step=30450\n",
            "2026-02-19 10:59:12 [debug    ] train_step                     loss=-3.089886 lr=7.96e-04 step=30500\n",
            "2026-02-19 10:59:15 [debug    ] train_step                     loss=-3.172066 lr=7.95e-04 step=30550\n",
            "2026-02-19 10:59:18 [debug    ] train_step                     loss=-3.136443 lr=7.95e-04 step=30600\n",
            "2026-02-19 10:59:21 [debug    ] train_step                     loss=-3.182985 lr=7.94e-04 step=30650\n",
            "2026-02-19 10:59:24 [debug    ] train_step                     loss=-3.070403 lr=7.93e-04 step=30700\n",
            "2026-02-19 10:59:27 [debug    ] train_step                     loss=-3.164132 lr=7.93e-04 step=30750\n",
            "2026-02-19 10:59:30 [debug    ] train_step                     loss=-2.847131 lr=7.92e-04 step=30800\n",
            "2026-02-19 10:59:33 [debug    ] train_step                     loss=-2.958028 lr=7.92e-04 step=30850\n",
            "2026-02-19 10:59:36 [debug    ] train_step                     loss=-2.914587 lr=7.91e-04 step=30900\n",
            "2026-02-19 10:59:39 [debug    ] train_step                     loss=-3.073781 lr=7.90e-04 step=30950\n",
            "2026-02-19 10:59:41 [debug    ] train_step                     loss=-3.054143 lr=7.90e-04 step=31000\n",
            "2026-02-19 10:59:44 [debug    ] train_step                     loss=-3.041807 lr=7.89e-04 step=31050\n",
            "2026-02-19 10:59:47 [debug    ] train_step                     loss=-2.826115 lr=7.88e-04 step=31100\n",
            "2026-02-19 10:59:50 [debug    ] train_step                     loss=-2.960629 lr=7.88e-04 step=31150\n",
            "2026-02-19 10:59:53 [info     ] epoch_complete                 epoch=31 lr=7.88e-04 train_loss=-3.038626 val_loss=-3.217915\n",
            "2026-02-19 10:59:57 [debug    ] train_step                     loss=-2.830318 lr=7.87e-04 step=31200\n",
            "2026-02-19 11:00:00 [debug    ] train_step                     loss=-3.134508 lr=7.86e-04 step=31250\n",
            "2026-02-19 11:00:03 [debug    ] train_step                     loss=-3.044702 lr=7.86e-04 step=31300\n",
            "2026-02-19 11:00:06 [debug    ] train_step                     loss=-3.012538 lr=7.85e-04 step=31350\n",
            "2026-02-19 11:00:09 [debug    ] train_step                     loss=-2.701589 lr=7.85e-04 step=31400\n",
            "2026-02-19 11:00:12 [debug    ] train_step                     loss=-3.222436 lr=7.84e-04 step=31450\n",
            "2026-02-19 11:00:15 [debug    ] train_step                     loss=-3.000834 lr=7.83e-04 step=31500\n",
            "2026-02-19 11:00:18 [debug    ] train_step                     loss=-2.959673 lr=7.83e-04 step=31550\n",
            "2026-02-19 11:00:21 [debug    ] train_step                     loss=-3.155173 lr=7.82e-04 step=31600\n",
            "2026-02-19 11:00:24 [debug    ] train_step                     loss=-3.025084 lr=7.81e-04 step=31650\n",
            "2026-02-19 11:00:27 [debug    ] train_step                     loss=-3.053718 lr=7.81e-04 step=31700\n",
            "2026-02-19 11:00:30 [debug    ] train_step                     loss=-2.915370 lr=7.80e-04 step=31750\n",
            "2026-02-19 11:00:33 [debug    ] train_step                     loss=-3.032094 lr=7.79e-04 step=31800\n",
            "2026-02-19 11:00:36 [debug    ] train_step                     loss=-3.159139 lr=7.79e-04 step=31850\n",
            "2026-02-19 11:00:38 [debug    ] train_step                     loss=-3.177016 lr=7.78e-04 step=31900\n",
            "2026-02-19 11:00:41 [debug    ] train_step                     loss=-3.129009 lr=7.77e-04 step=31950\n",
            "2026-02-19 11:00:44 [debug    ] train_step                     loss=-2.987113 lr=7.77e-04 step=32000\n",
            "2026-02-19 11:00:47 [debug    ] train_step                     loss=-3.057883 lr=7.76e-04 step=32050\n",
            "2026-02-19 11:00:50 [debug    ] train_step                     loss=-2.825365 lr=7.75e-04 step=32100\n",
            "2026-02-19 11:00:53 [debug    ] train_step                     loss=-3.078678 lr=7.75e-04 step=32150\n",
            "2026-02-19 11:00:56 [info     ] epoch_complete                 epoch=32 lr=7.75e-04 train_loss=-3.032529 val_loss=-3.202963\n",
            "2026-02-19 11:01:00 [debug    ] train_step                     loss=-2.979621 lr=7.74e-04 step=32200\n",
            "2026-02-19 11:01:03 [debug    ] train_step                     loss=-3.094196 lr=7.74e-04 step=32250\n",
            "2026-02-19 11:01:06 [debug    ] train_step                     loss=-3.109341 lr=7.73e-04 step=32300\n",
            "2026-02-19 11:01:09 [debug    ] train_step                     loss=-3.236512 lr=7.72e-04 step=32350\n",
            "2026-02-19 11:01:11 [debug    ] train_step                     loss=-2.936912 lr=7.72e-04 step=32400\n",
            "2026-02-19 11:01:14 [debug    ] train_step                     loss=-2.992717 lr=7.71e-04 step=32450\n",
            "2026-02-19 11:01:17 [debug    ] train_step                     loss=-3.225115 lr=7.70e-04 step=32500\n",
            "2026-02-19 11:01:20 [debug    ] train_step                     loss=-3.206163 lr=7.70e-04 step=32550\n",
            "2026-02-19 11:01:23 [debug    ] train_step                     loss=-3.202067 lr=7.69e-04 step=32600\n",
            "2026-02-19 11:01:26 [debug    ] train_step                     loss=-3.177644 lr=7.68e-04 step=32650\n",
            "2026-02-19 11:01:29 [debug    ] train_step                     loss=-3.202633 lr=7.68e-04 step=32700\n",
            "2026-02-19 11:01:32 [debug    ] train_step                     loss=-2.941145 lr=7.67e-04 step=32750\n",
            "2026-02-19 11:01:35 [debug    ] train_step                     loss=-2.931957 lr=7.66e-04 step=32800\n",
            "2026-02-19 11:01:38 [debug    ] train_step                     loss=-2.946270 lr=7.66e-04 step=32850\n",
            "2026-02-19 11:01:41 [debug    ] train_step                     loss=-3.158536 lr=7.65e-04 step=32900\n",
            "2026-02-19 11:01:44 [debug    ] train_step                     loss=-3.056374 lr=7.64e-04 step=32950\n",
            "2026-02-19 11:01:47 [debug    ] train_step                     loss=-3.012860 lr=7.64e-04 step=33000\n",
            "2026-02-19 11:01:50 [debug    ] train_step                     loss=-3.181278 lr=7.63e-04 step=33050\n",
            "2026-02-19 11:01:53 [debug    ] train_step                     loss=-2.982638 lr=7.62e-04 step=33100\n",
            "2026-02-19 11:01:56 [debug    ] train_step                     loss=-3.041102 lr=7.62e-04 step=33150\n",
            "2026-02-19 11:01:59 [info     ] epoch_complete                 epoch=33 lr=7.61e-04 train_loss=-3.031765 val_loss=-3.132864\n",
            "2026-02-19 11:02:02 [debug    ] train_step                     loss=-3.019921 lr=7.61e-04 step=33200\n",
            "2026-02-19 11:02:05 [debug    ] train_step                     loss=-2.990659 lr=7.60e-04 step=33250\n",
            "2026-02-19 11:02:08 [debug    ] train_step                     loss=-3.035629 lr=7.60e-04 step=33300\n",
            "2026-02-19 11:02:11 [debug    ] train_step                     loss=-3.112116 lr=7.59e-04 step=33350\n",
            "2026-02-19 11:02:14 [debug    ] train_step                     loss=-3.098686 lr=7.58e-04 step=33400\n",
            "2026-02-19 11:02:17 [debug    ] train_step                     loss=-3.071507 lr=7.58e-04 step=33450\n",
            "2026-02-19 11:02:20 [debug    ] train_step                     loss=-3.068240 lr=7.57e-04 step=33500\n",
            "2026-02-19 11:02:23 [debug    ] train_step                     loss=-3.091396 lr=7.56e-04 step=33550\n",
            "2026-02-19 11:02:26 [debug    ] train_step                     loss=-3.029229 lr=7.56e-04 step=33600\n",
            "2026-02-19 11:02:29 [debug    ] train_step                     loss=-3.076908 lr=7.55e-04 step=33650\n",
            "2026-02-19 11:02:32 [debug    ] train_step                     loss=-3.047630 lr=7.54e-04 step=33700\n",
            "2026-02-19 11:02:35 [debug    ] train_step                     loss=-3.009575 lr=7.54e-04 step=33750\n",
            "2026-02-19 11:02:38 [debug    ] train_step                     loss=-3.169684 lr=7.53e-04 step=33800\n",
            "2026-02-19 11:02:41 [debug    ] train_step                     loss=-3.098405 lr=7.52e-04 step=33850\n",
            "2026-02-19 11:02:44 [debug    ] train_step                     loss=-3.040841 lr=7.52e-04 step=33900\n",
            "2026-02-19 11:02:47 [debug    ] train_step                     loss=-3.175950 lr=7.51e-04 step=33950\n",
            "2026-02-19 11:02:50 [debug    ] train_step                     loss=-2.865083 lr=7.50e-04 step=34000\n",
            "2026-02-19 11:02:53 [debug    ] train_step                     loss=-2.776071 lr=7.50e-04 step=34050\n",
            "2026-02-19 11:02:56 [debug    ] train_step                     loss=-3.152045 lr=7.49e-04 step=34100\n",
            "2026-02-19 11:02:59 [debug    ] train_step                     loss=-2.914665 lr=7.48e-04 step=34150\n",
            "2026-02-19 11:03:02 [info     ] epoch_complete                 epoch=34 lr=7.48e-04 train_loss=-3.036715 val_loss=-3.153567\n",
            "2026-02-19 11:03:05 [debug    ] train_step                     loss=-3.021933 lr=7.48e-04 step=34200\n",
            "2026-02-19 11:03:08 [debug    ] train_step                     loss=-2.912755 lr=7.47e-04 step=34250\n",
            "2026-02-19 11:03:11 [debug    ] train_step                     loss=-3.136230 lr=7.46e-04 step=34300\n",
            "2026-02-19 11:03:14 [debug    ] train_step                     loss=-3.161104 lr=7.46e-04 step=34350\n",
            "2026-02-19 11:03:17 [debug    ] train_step                     loss=-2.999560 lr=7.45e-04 step=34400\n",
            "2026-02-19 11:03:20 [debug    ] train_step                     loss=-3.153600 lr=7.44e-04 step=34450\n",
            "2026-02-19 11:03:23 [debug    ] train_step                     loss=-3.040089 lr=7.43e-04 step=34500\n",
            "2026-02-19 11:03:26 [debug    ] train_step                     loss=-2.995120 lr=7.43e-04 step=34550\n",
            "2026-02-19 11:03:29 [debug    ] train_step                     loss=-3.234059 lr=7.42e-04 step=34600\n",
            "2026-02-19 11:03:32 [debug    ] train_step                     loss=-3.095802 lr=7.41e-04 step=34650\n",
            "2026-02-19 11:03:35 [debug    ] train_step                     loss=-2.839471 lr=7.41e-04 step=34700\n",
            "2026-02-19 11:03:38 [debug    ] train_step                     loss=-3.049087 lr=7.40e-04 step=34750\n",
            "2026-02-19 11:03:41 [debug    ] train_step                     loss=-3.199303 lr=7.39e-04 step=34800\n",
            "2026-02-19 11:03:44 [debug    ] train_step                     loss=-3.153587 lr=7.39e-04 step=34850\n",
            "2026-02-19 11:03:47 [debug    ] train_step                     loss=-3.180514 lr=7.38e-04 step=34900\n",
            "2026-02-19 11:03:50 [debug    ] train_step                     loss=-2.785275 lr=7.37e-04 step=34950\n",
            "2026-02-19 11:03:53 [debug    ] train_step                     loss=-3.298819 lr=7.37e-04 step=35000\n",
            "2026-02-19 11:03:56 [debug    ] train_step                     loss=-2.979876 lr=7.36e-04 step=35050\n",
            "2026-02-19 11:03:58 [debug    ] train_step                     loss=-3.044256 lr=7.35e-04 step=35100\n",
            "2026-02-19 11:04:01 [debug    ] train_step                     loss=-2.790878 lr=7.35e-04 step=35150\n",
            "2026-02-19 11:04:05 [info     ] epoch_complete                 epoch=35 lr=7.34e-04 train_loss=-3.041018 val_loss=-3.099295\n",
            "2026-02-19 11:04:08 [debug    ] train_step                     loss=-3.099494 lr=7.34e-04 step=35200\n",
            "2026-02-19 11:04:11 [debug    ] train_step                     loss=-3.165442 lr=7.33e-04 step=35250\n",
            "2026-02-19 11:04:14 [debug    ] train_step                     loss=-3.129593 lr=7.32e-04 step=35300\n",
            "2026-02-19 11:04:17 [debug    ] train_step                     loss=-2.842185 lr=7.32e-04 step=35350\n",
            "2026-02-19 11:04:20 [debug    ] train_step                     loss=-3.107506 lr=7.31e-04 step=35400\n",
            "2026-02-19 11:04:23 [debug    ] train_step                     loss=-2.936577 lr=7.30e-04 step=35450\n",
            "2026-02-19 11:04:26 [debug    ] train_step                     loss=-3.086360 lr=7.30e-04 step=35500\n",
            "2026-02-19 11:04:29 [debug    ] train_step                     loss=-3.061242 lr=7.29e-04 step=35550\n",
            "2026-02-19 11:04:32 [debug    ] train_step                     loss=-3.173429 lr=7.28e-04 step=35600\n",
            "2026-02-19 11:04:35 [debug    ] train_step                     loss=-3.049596 lr=7.28e-04 step=35650\n",
            "2026-02-19 11:04:38 [debug    ] train_step                     loss=-2.993902 lr=7.27e-04 step=35700\n",
            "2026-02-19 11:04:41 [debug    ] train_step                     loss=-3.092561 lr=7.26e-04 step=35750\n",
            "2026-02-19 11:04:44 [debug    ] train_step                     loss=-3.106338 lr=7.26e-04 step=35800\n",
            "2026-02-19 11:04:46 [debug    ] train_step                     loss=-3.116647 lr=7.25e-04 step=35850\n",
            "2026-02-19 11:04:49 [debug    ] train_step                     loss=-3.143682 lr=7.24e-04 step=35900\n",
            "2026-02-19 11:04:52 [debug    ] train_step                     loss=-3.190144 lr=7.23e-04 step=35950\n",
            "2026-02-19 11:04:55 [debug    ] train_step                     loss=-3.179036 lr=7.23e-04 step=36000\n",
            "2026-02-19 11:04:58 [debug    ] train_step                     loss=-3.114872 lr=7.22e-04 step=36050\n",
            "2026-02-19 11:05:01 [debug    ] train_step                     loss=-2.022929 lr=7.21e-04 step=36100\n",
            "2026-02-19 11:05:04 [debug    ] train_step                     loss=-3.042933 lr=7.21e-04 step=36150\n",
            "2026-02-19 11:05:08 [info     ] epoch_complete                 epoch=36 lr=7.20e-04 train_loss=-3.040503 val_loss=-3.202741\n",
            "2026-02-19 11:05:11 [debug    ] train_step                     loss=-2.941522 lr=7.20e-04 step=36200\n",
            "2026-02-19 11:05:14 [debug    ] train_step                     loss=-3.070012 lr=7.19e-04 step=36250\n",
            "2026-02-19 11:05:17 [debug    ] train_step                     loss=-3.268586 lr=7.19e-04 step=36300\n",
            "2026-02-19 11:05:20 [debug    ] train_step                     loss=-3.171240 lr=7.18e-04 step=36350\n",
            "2026-02-19 11:05:23 [debug    ] train_step                     loss=-3.044960 lr=7.17e-04 step=36400\n",
            "2026-02-19 11:05:26 [debug    ] train_step                     loss=-3.129410 lr=7.16e-04 step=36450\n",
            "2026-02-19 11:05:29 [debug    ] train_step                     loss=-2.922953 lr=7.16e-04 step=36500\n",
            "2026-02-19 11:05:32 [debug    ] train_step                     loss=-3.000435 lr=7.15e-04 step=36550\n",
            "2026-02-19 11:05:35 [debug    ] train_step                     loss=-3.126499 lr=7.14e-04 step=36600\n",
            "2026-02-19 11:05:38 [debug    ] train_step                     loss=-3.246404 lr=7.14e-04 step=36650\n",
            "2026-02-19 11:05:41 [debug    ] train_step                     loss=-2.899981 lr=7.13e-04 step=36700\n",
            "2026-02-19 11:05:44 [debug    ] train_step                     loss=-2.782720 lr=7.12e-04 step=36750\n",
            "2026-02-19 11:05:47 [debug    ] train_step                     loss=-3.241257 lr=7.12e-04 step=36800\n",
            "2026-02-19 11:05:50 [debug    ] train_step                     loss=-3.141837 lr=7.11e-04 step=36850\n",
            "2026-02-19 11:05:52 [debug    ] train_step                     loss=-2.913351 lr=7.10e-04 step=36900\n",
            "2026-02-19 11:05:55 [debug    ] train_step                     loss=-2.740259 lr=7.09e-04 step=36950\n",
            "2026-02-19 11:05:58 [debug    ] train_step                     loss=-3.126287 lr=7.09e-04 step=37000\n",
            "2026-02-19 11:06:01 [debug    ] train_step                     loss=-3.162651 lr=7.08e-04 step=37050\n",
            "2026-02-19 11:06:04 [debug    ] train_step                     loss=-3.058940 lr=7.07e-04 step=37100\n",
            "2026-02-19 11:06:07 [debug    ] train_step                     loss=-2.894991 lr=7.07e-04 step=37150\n",
            "2026-02-19 11:06:11 [info     ] epoch_complete                 epoch=37 lr=7.06e-04 train_loss=-3.043925 val_loss=-3.178045\n",
            "2026-02-19 11:06:14 [debug    ] train_step                     loss=-3.160416 lr=7.06e-04 step=37200\n",
            "2026-02-19 11:06:17 [debug    ] train_step                     loss=-2.967695 lr=7.05e-04 step=37250\n",
            "2026-02-19 11:06:19 [debug    ] train_step                     loss=-3.100822 lr=7.04e-04 step=37300\n",
            "2026-02-19 11:06:22 [debug    ] train_step                     loss=-3.146355 lr=7.04e-04 step=37350\n",
            "2026-02-19 11:06:25 [debug    ] train_step                     loss=-3.190892 lr=7.03e-04 step=37400\n",
            "2026-02-19 11:06:28 [debug    ] train_step                     loss=-3.214473 lr=7.02e-04 step=37450\n",
            "2026-02-19 11:06:31 [debug    ] train_step                     loss=-1.615232 lr=7.02e-04 step=37500\n",
            "2026-02-19 11:06:34 [debug    ] train_step                     loss=-2.996606 lr=7.01e-04 step=37550\n",
            "2026-02-19 11:06:37 [debug    ] train_step                     loss=-3.275525 lr=7.00e-04 step=37600\n",
            "2026-02-19 11:06:40 [debug    ] train_step                     loss=-3.053862 lr=6.99e-04 step=37650\n",
            "2026-02-19 11:06:43 [debug    ] train_step                     loss=-2.886993 lr=6.99e-04 step=37700\n",
            "2026-02-19 11:06:46 [debug    ] train_step                     loss=-3.111361 lr=6.98e-04 step=37750\n",
            "2026-02-19 11:06:49 [debug    ] train_step                     loss=-2.955465 lr=6.97e-04 step=37800\n",
            "2026-02-19 11:06:52 [debug    ] train_step                     loss=-3.124681 lr=6.97e-04 step=37850\n",
            "2026-02-19 11:06:55 [debug    ] train_step                     loss=-3.236181 lr=6.96e-04 step=37900\n",
            "2026-02-19 11:06:58 [debug    ] train_step                     loss=-3.226013 lr=6.95e-04 step=37950\n",
            "2026-02-19 11:07:01 [debug    ] train_step                     loss=-3.213813 lr=6.94e-04 step=38000\n",
            "2026-02-19 11:07:04 [debug    ] train_step                     loss=-2.919902 lr=6.94e-04 step=38050\n",
            "2026-02-19 11:07:07 [debug    ] train_step                     loss=-3.091621 lr=6.93e-04 step=38100\n",
            "2026-02-19 11:07:10 [debug    ] train_step                     loss=-3.094228 lr=6.92e-04 step=38150\n",
            "2026-02-19 11:07:15 [info     ] epoch_complete                 epoch=38 lr=6.92e-04 train_loss=-3.038316 val_loss=-3.231780\n",
            "2026-02-19 11:07:16 [info     ] early_stopping                 best_score=-3.2346034241934953 patience=15\n",
            "2026-02-19 11:07:16 [info     ] early_stopping_triggered       epoch=38\n",
            "2026-02-19 11:07:16 [info     ] training_complete              epochs_run=38\n",
            "\n",
            "TFT training done in 40.1 min\n",
            "Final train loss: -3.038316\n",
            "Best val loss: -3.234603\n",
            "TFT model saved to Drive!\n"
          ]
        }
      ],
      "execution_count": 13,
      "id": "bmhgCL92BG5o"
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "import quant_lab.models.tft.model as tft_model\n",
        "\n",
        "print([name for name, obj in inspect.getmembers(tft_model)\n",
        "       if inspect.isclass(obj)])\n"
      ],
      "metadata": {
        "id": "tLX3TVzxZ53W"
      },
      "id": "tLX3TVzxZ53W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7I_nGpLBG5p"
      },
      "source": [
        "## RL Portfolio Allocation (PPO)\n",
        "Proximal Policy Optimization with portfolio environment\n",
        "- Reward = Sharpe - MDD penalty - trading costs - turnover penalty\n",
        "- H100: 2M timesteps"
      ],
      "id": "f7I_nGpLBG5p"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iLExKKvBG5s",
        "outputId": "a163c163-49fe-4783-ba04-b819904fd600"
      },
      "source": [
        "# === RL PPO TRAINING (H100 optimized) ===\n",
        "import time\n",
        "import numpy as np\n",
        "from quant_lab.rl.environments.portfolio_env import PortfolioEnvConfig\n",
        "from quant_lab.rl.environments.reward import RewardConfig\n",
        "from quant_lab.rl.training import train_rl, RLTrainingConfig\n",
        "\n",
        "set_global_seed(42)\n",
        "\n",
        "# Build feature tensors\n",
        "base_cols = {'date', 'ticker', 'open', 'high', 'low', 'close', 'volume', 'adj_close'}\n",
        "feat_cols = [c for c in feature_df.columns if c not in base_cols]\n",
        "\n",
        "def build_feature_tensor(df, feat_cols, start, end):\n",
        "    import pandas as pd\n",
        "    df = df.copy()\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df = df[(df['date'] > start) & (df['date'] <= end)]\n",
        "    if 'log_return_1d' not in df.columns:\n",
        "        df['log_return_1d'] = df.groupby('ticker')['adj_close'].transform(lambda s: np.log(s / s.shift(1)))\n",
        "    tickers = sorted(df['ticker'].unique())\n",
        "    dates = sorted(df['date'].unique())\n",
        "    features = np.zeros((len(dates), len(tickers), len(feat_cols)), dtype=np.float32)\n",
        "    returns = np.zeros((len(dates), len(tickers)), dtype=np.float32)\n",
        "    t_map = {t: i for i, t in enumerate(tickers)}\n",
        "    d_map = {d: i for i, d in enumerate(dates)}\n",
        "    for _, row in df.iterrows():\n",
        "        ti, di = d_map[row['date']], t_map[row['ticker']]\n",
        "        features[ti, di, :] = row[feat_cols].values.astype(np.float32)\n",
        "        ret = row.get('log_return_1d', 0.0)\n",
        "        returns[ti, di] = 0.0 if pd.isna(ret) else float(ret)\n",
        "    return np.nan_to_num(features, nan=0.0), returns\n",
        "\n",
        "train_features, train_returns = build_feature_tensor(feature_df, feat_cols, '1900-01-01', '2021-12-31')\n",
        "val_features, val_returns = build_feature_tensor(feature_df, feat_cols, '2021-12-31', '2023-06-30')\n",
        "print(f\"Train: {train_features.shape}, Val: {val_features.shape}\")\n",
        "\n",
        "env_config = PortfolioEnvConfig(initial_cash=1_000_000, max_weight=0.20, rebalance_frequency=5)\n",
        "reward_config = RewardConfig(lambda_mdd=0.5, lambda_turnover=0.01, commission_bps=10.0, slippage_bps=5.0, spread_bps=5.0)\n",
        "training_config = RLTrainingConfig(\n",
        "    algorithm='ppo',\n",
        "    total_timesteps=2_000_000,\n",
        "    eval_freq=50_000,\n",
        "    n_eval_episodes=5,\n",
        "    checkpoint_dir='outputs/models/rl/ppo',\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "result = train_rl(\n",
        "    train_features=train_features, train_returns=train_returns,\n",
        "    val_features=val_features, val_returns=val_returns,\n",
        "    config=training_config, env_config=env_config, reward_config=reward_config,\n",
        "    device='auto',\n",
        ")\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"\\nPPO training done in {elapsed/60:.1f} min\")\n",
        "for k, v in result['train_metrics'].items():\n",
        "    print(f\"  Train {k}: {v:.4f}\")\n",
        "\n",
        "# Save to Drive\n",
        "import shutil\n",
        "for f in Path('outputs/models/rl/ppo').glob('*'):\n",
        "    (DRIVE_DIR / 'outputs/models/rl/ppo').mkdir(parents=True, exist_ok=True)\n",
        "    shutil.copy(f, DRIVE_DIR / 'outputs/models/rl/ppo' / f.name)\n",
        "print(\"PPO agent saved to Drive!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (2962, 49, 15), Val: (370, 49, 15)\n",
            "2026-02-19 11:08:27 [info     ] ppo_agent_created              lr=0.0003 policy=MlpPolicy\n",
            "2026-02-19 11:08:27 [info     ] rl_training_start              algorithm=ppo num_assets=49 num_steps=2962 total_timesteps=2000000\n",
            "2026-02-19 11:08:27 [info     ] ppo_training_start             total_timesteps=2000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-19 12:13:38 [info     ] ppo_training_complete\n",
            "2026-02-19 12:13:47 [info     ] rl_train_eval                  mean_final_value=9686950.561765622 mean_reward=-83.49495207718385 std_final_value=0.0 std_reward=0.0\n",
            "2026-02-19 12:13:48 [info     ] rl_val_eval                    mean_final_value=1220236.5429536235 mean_reward=-10.896220953021965 std_final_value=0.0 std_reward=0.0\n",
            "2026-02-19 12:13:48 [info     ] ppo_saved                      path=outputs/models/rl/ppo/ppo_agent\n",
            "2026-02-19 12:13:48 [info     ] rl_training_complete\n",
            "\n",
            "PPO training done in 65.4 min\n",
            "  Train mean_reward: -83.4950\n",
            "  Train std_reward: 0.0000\n",
            "  Train mean_final_value: 9686950.5618\n",
            "  Train std_final_value: 0.0000\n",
            "PPO agent saved to Drive!\n"
          ]
        }
      ],
      "execution_count": 14,
      "id": "4iLExKKvBG5s"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSuUZ01cBG5v",
        "outputId": "4e3ee388-75e1-4258-ce5b-6157045daa2a"
      },
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"NOTEBOOK B COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"TFT final loss: {history['train_loss'][-1]:.6f}\")\n",
        "print(f\"PPO metrics: {result['train_metrics']}\")\n",
        "print(f\"\\nModels on Drive:\")\n",
        "for d in ['outputs/models/tft', 'outputs/models/rl/ppo']:\n",
        "    p = DRIVE_DIR / d\n",
        "    if p.exists():\n",
        "        for f in p.glob('*'):\n",
        "            print(f\"  {f.relative_to(DRIVE_DIR)}: {f.stat().st_size/1e6:.1f} MB\")\n",
        "print(\"=\" * 60)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "NOTEBOOK B COMPLETE\n",
            "============================================================\n",
            "TFT final loss: -3.038316\n",
            "PPO metrics: {'mean_reward': -83.49495207718385, 'std_reward': 0.0, 'mean_final_value': 9686950.561765622, 'std_final_value': 0.0}\n",
            "\n",
            "Models on Drive:\n",
            "  outputs/models/tft/last.pt: 427.5 MB\n",
            "  outputs/models/tft/best.pt: 427.5 MB\n",
            "  outputs/models/tft/final_model.pt: 142.5 MB\n",
            "  outputs/models/rl/ppo/ppo_agent.zip: 1.4 MB\n",
            "============================================================\n"
          ]
        }
      ],
      "execution_count": 15,
      "id": "QSuUZ01cBG5v"
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Re-define DRIVE_DIR for scope safety if this cell runs independently\n",
        "# Assumes DRIVE_DIR is already defined by previous setup cells\n",
        "# If not, uncomment and set it:\n",
        "# DRIVE_DIR = Path('/content/drive/MyDrive/quant_lab')\n",
        "\n",
        "local_outputs_root = Path('outputs') # Assumes current working directory is /content/quant-lab\n",
        "drive_outputs_root = DRIVE_DIR / 'outputs'\n",
        "\n",
        "# Ensure the root outputs directory on Drive exists\n",
        "drive_outputs_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Checking for outputs in '{local_outputs_root}' to save to Drive at '{drive_outputs_root}'...\")\n",
        "\n",
        "# Handle MLflow logs explicitly\n",
        "local_mlruns_path = local_outputs_root / 'mlruns'\n",
        "drive_mlruns_path = drive_outputs_root / 'mlruns'\n",
        "\n",
        "if local_mlruns_path.exists():\n",
        "    if not drive_mlruns_path.exists():\n",
        "        print(f\"  Copying new MLflow logs directory '{local_mlruns_path.name}' to Drive...\")\n",
        "        shutil.copytree(local_mlruns_path, drive_mlruns_path)\n",
        "    else:\n",
        "        print(f\"  MLflow logs directory '{local_mlruns_path.name}' already exists on Drive. Merging content...\")\n",
        "        # A simple merge: iterate through local mlruns and copy files, overwriting if newer\n",
        "        for src_dir, dirs, files in os.walk(local_mlruns_path):\n",
        "            relative_path = Path(src_dir).relative_to(local_mlruns_path)\n",
        "            dst_dir = drive_mlruns_path / relative_path\n",
        "            dst_dir.mkdir(parents=True, exist_ok=True)\n",
        "            for file_name in files:\n",
        "                shutil.copy2(Path(src_dir) / file_name, dst_dir / file_name)\n",
        "        print(f\"  MLflow logs content from '{local_mlruns_path.name}' merged to Drive.\")\n",
        "else:\n",
        "    print(f\"  No local MLflow logs directory found at '{local_mlruns_path}'.\")\n",
        "\n",
        "# Handle any other direct files/directories in 'outputs/' not covered by 'models'\n",
        "# (assuming 'models' contents are handled by previous cells' explicit copies)\n",
        "if local_outputs_root.exists():\n",
        "    for item in local_outputs_root.iterdir():\n",
        "        if item.name in ['models', 'mlruns']:\n",
        "            continue # Already handled or explicitly skipped\n",
        "\n",
        "        drive_item_path = drive_outputs_root / item.name\n",
        "\n",
        "        if item.is_dir():\n",
        "            if not drive_item_path.exists():\n",
        "                print(f\"  Copying other directory '{item.name}' to Drive...\")\n",
        "                shutil.copytree(item, drive_item_path)\n",
        "            else:\n",
        "                print(f\"  Directory '{item.name}' already exists on Drive. Skipping. Manual sync may be needed.\")\n",
        "        elif item.is_file():\n",
        "            print(f\"  Copying other file '{item.name}' to Drive...\")\n",
        "            shutil.copy2(item, drive_item_path)\n",
        "\n",
        "print(\"Finished ensuring all relevant outputs are saved to Drive.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hPYC0h02KvS",
        "outputId": "5964bf2a-6f77-4a8d-930e-502678cfe838"
      },
      "id": "7hPYC0h02KvS",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for outputs in 'outputs' to save to Drive at '/content/drive/MyDrive/quant_lab/outputs'...\n",
            "  No local MLflow logs directory found at 'outputs/mlruns'.\n",
            "Finished ensuring all relevant outputs are saved to Drive.\n"
          ]
        }
      ]
    }
  ]
}